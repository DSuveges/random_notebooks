{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking ClinGen vs Literature evidence\n",
    "\n",
    "\n",
    "1. Fetch most recent OT envidence set\n",
    "2. Parse evidence: get all literature based evidence.\n",
    "3. Apply score filter.\n",
    "\n",
    "```bash\n",
    "cd /Users/dsuveges/project/evidences\n",
    "wget https://storage.googleapis.com/open-targets-data-releases/20.06/output/20.06_evidence_data.json.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing evidence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T08:17:05.670816Z",
     "start_time": "2020-08-19T08:17:01.142074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of literature evidence: 47,300\n",
      "Number of unique target/disease evidence: 27,402\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "literature_data = '/Users/dsuveges/project/evidences/20.06_evidence_data.filtered.parquet'\n",
    "clingen_Data = '/Users/dsuveges/project/evidences/clingen_2020-08-04.json.gz'\n",
    "\n",
    "# Reading and filtering literature data:\n",
    "literature_df = pd.read_parquet(literature_data)\n",
    "\n",
    "score_threshold = 0.6 # 32570 evidence / 20289 unique\n",
    "score_threshold = 0.5 # 47300 evidence / 27402 unique\n",
    "\n",
    "filtered_lit = literature_df.loc[literature_df.assoc_score >= score_threshold]\n",
    "\n",
    "# Opening filtered and processed clingen data:\n",
    "clingen_df = pd.read_csv('parsed_clingen_data.tsv.gz', sep='\\t', compression='infer')\n",
    "\n",
    "print(f\"Number of literature evidence: {len(filtered_lit):,}\")\n",
    "print(f\"Number of unique target/disease evidence: {len(filtered_lit[['efo_code','gene_id']].drop_duplicates()):,}\")\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T15:04:24.693363Z",
     "start_time": "2020-08-18T15:04:24.673036Z"
    }
   },
   "source": [
    "### Find out overlap between clingen and the literature evidence set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:57:18.679184Z",
     "start_time": "2020-08-19T07:57:18.645866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Definitive</td>\n",
       "      <td>528</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Strong</td>\n",
       "      <td>19</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moderate</td>\n",
       "      <td>100</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limited</td>\n",
       "      <td>162</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disputed</td>\n",
       "      <td>69</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Refuted</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Reported Evidence</td>\n",
       "      <td>92</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             confidence  evidence_count  score\n",
       "0            Definitive             528   1.00\n",
       "6                Strong              19   0.75\n",
       "3              Moderate             100   0.50\n",
       "2               Limited             162   0.25\n",
       "1              Disputed              69   0.10\n",
       "5               Refuted              10   0.05\n",
       "4  No Reported Evidence              92   0.01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clingen_data.apply(lambda row: (row['confidence'],row['score']), axis=1).value_counts()\n",
    "\n",
    "summary_list = []\n",
    "for conf, group in clingen_df.groupby(['confidence']):\n",
    "    summary_list.append({\n",
    "        'confidence': conf,\n",
    "        'evidence_count': len(group),\n",
    "        'score': group.score.tolist()[0]\n",
    "    })\n",
    "    \n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "summary_df.sort_values(['score'],ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T07:57:29.966689Z",
     "start_time": "2020-08-19T07:57:25.223704Z"
    }
   },
   "outputs": [],
   "source": [
    "confidence = 'Definitive'\n",
    "\n",
    "def get_overlap(confidence):\n",
    "\n",
    "    # Filter data for a specific confidence:\n",
    "    pairs = clingen_df.loc[clingen_df.confidence == confidence]\n",
    "\n",
    "    # Filter for overlapping target/disease pairs:\n",
    "    found_scores = pairs.apply(lambda row: filtered_lit.loc[(filtered_lit.efo_code == row['disease']) & (filtered_lit.gene_id == row['target'])].assoc_score.mean(), axis =1)\n",
    "\n",
    "    # Report overlap:\n",
    "    score_mean = found_scores.loc[~found_scores.isna()].mean() # Found scores can be plot as boxplot\n",
    "    hits_found = len(found_scores.loc[~found_scores.isna()])\n",
    "    \n",
    "    return (hits_found, score_mean)\n",
    "\n",
    "overlaps = summary_df.confidence.apply(get_overlap)\n",
    "\n",
    "summary_df['lit_overlap_cnt'] = overlaps.apply(lambda x: x[0])\n",
    "summary_df['lit_overlap_mean_score'] = overlaps.apply(lambda x: x[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T15:28:48.098940Z",
     "start_time": "2020-08-18T15:28:48.085245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confidence</th>\n",
       "      <th>evidence_count</th>\n",
       "      <th>score</th>\n",
       "      <th>lit_overlap_cnt</th>\n",
       "      <th>lit_overlap_mean_score</th>\n",
       "      <th>overlap_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Definitive</td>\n",
       "      <td>528</td>\n",
       "      <td>1.00</td>\n",
       "      <td>32</td>\n",
       "      <td>0.659018</td>\n",
       "      <td>0.060606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Strong</td>\n",
       "      <td>19</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moderate</td>\n",
       "      <td>100</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limited</td>\n",
       "      <td>162</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Disputed</td>\n",
       "      <td>69</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>0.014493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Refuted</td>\n",
       "      <td>10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Reported Evidence</td>\n",
       "      <td>92</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             confidence  evidence_count  score  lit_overlap_cnt  \\\n",
       "0            Definitive             528   1.00               32   \n",
       "6                Strong              19   0.75                1   \n",
       "3              Moderate             100   0.50                1   \n",
       "2               Limited             162   0.25                0   \n",
       "1              Disputed              69   0.10                1   \n",
       "5               Refuted              10   0.05                1   \n",
       "4  No Reported Evidence              92   0.01                0   \n",
       "\n",
       "   lit_overlap_mean_score  overlap_prop  \n",
       "0                0.659018      0.060606  \n",
       "6                0.504000      0.052632  \n",
       "3                0.690000      0.010000  \n",
       "2                     NaN      0.000000  \n",
       "1                0.612000      0.014493  \n",
       "5                0.614000      0.100000  \n",
       "4                     NaN      0.000000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df['overlap_prop'] = summary_df.apply(lambda x: x['lit_overlap_cnt']/x['evidence_count'], axis = 1)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T08:14:30.450372Z",
     "start_time": "2020-08-19T08:14:30.445556Z"
    }
   },
   "source": [
    "## Test against all literature data\n",
    "\n",
    "\n",
    "Instead of applying a score threshold, I'll use all literature evidence as follows:\n",
    "\n",
    "1. Calculate harmonic sum for all unique target-disease pairs.\n",
    "2. Performace of ClinGen scores are tested against this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T10:12:25.686140Z",
     "start_time": "2020-08-19T10:12:25.681777Z"
    }
   },
   "outputs": [],
   "source": [
    "def harmonic_sum(data, scale_factor=1, cap=None):\n",
    "    \"\"\"\n",
    "    Returns an harmonic sum for the data passed\n",
    "    Args:\n",
    "        data (list): list of floats to compute the harmonic sum from\n",
    "        scale_factor (float): a scaling factor to multiply to each datapoint. Defaults to 1\n",
    "        cap (float): if not None, never return an harmonic sum higher than the cap value.\n",
    "    Returns:\n",
    "        harmonic_sum (float): the harmonic sum of the data passed\n",
    "    \"\"\"\n",
    "    \n",
    "    data.sort(reverse=True)\n",
    "    harmonic_sum = sum(s / ((i+1) ** scale_factor) for i, s in enumerate(data))\n",
    "    \n",
    "    # Applying cap:\n",
    "    if cap is not None and harmonic_sum > cap:\n",
    "        return cap\n",
    "      \n",
    "    return harmonic_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-19T10:17:00.068Z"
    }
   },
   "outputs": [],
   "source": [
    "summed_literature = pd.DataFrame(columns=['target','disease','score'])\n",
    "iloc = 0\n",
    "\n",
    "for (target, disease), group in literature_df.groupby(['gene_id', 'efo_code']):\n",
    "    summed_literature.loc[iloc,'target']  = target\n",
    "    summed_literature.loc[iloc,'disease'] = disease\n",
    "    summed_literature.loc[iloc,'score']   = harmonic_sum(group.assoc_score.tolist())\n",
    "    iloc += 1\n",
    "\n",
    "summed_literature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-19T10:16:52.130894Z",
     "start_time": "2020-08-19T10:16:52.127253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summed_literature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
