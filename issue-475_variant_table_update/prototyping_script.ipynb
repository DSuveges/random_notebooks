{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating variant table for OT Genetics Portal\n",
    "\n",
    "**Code base:** [genetics-variant-annotation](https://github.com/opentargets/genetics-variant-annotation)\n",
    "\n",
    "## Figure out hail\n",
    "\n",
    "1. What version should we use? -> 0.2 with the available spark version 2.4\n",
    "\n",
    "Get the corresponding hash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T20:36:51.024848Z",
     "start_time": "2021-07-17T20:36:46.452615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f3a54b5309797140ecd15811834132d9e1fafedf\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "HASH=$(gsutil cat gs://hail-common/builds/0.2/latest-hash/cloudtools-5-spark-2.4.0.txt)\n",
    "echo $HASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T09:40:14.625483Z",
     "start_time": "2021-07-19T09:40:14.620339Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_population_keys(pop_list):\n",
    "    ''' Takes a list of population names and filters to:\n",
    "        - Keep gnomad only\n",
    "        - Remove male/female sub-stats\n",
    "        - Remove _raw (stats before any sample filtering)\n",
    "    Params:\n",
    "        pop_list (list of str)\n",
    "    Returns:\n",
    "        Filtered pop_list (list of str)\n",
    "    '''\n",
    "    pop_filt = []\n",
    "    for pop in pop_list:\n",
    "        if (\n",
    "              pop.startswith('gnomad_') and not\n",
    "              pop.endswith('_raw') and not\n",
    "              pop.endswith('_male') and not\n",
    "              pop.endswith('_female')\n",
    "           ):\n",
    "           pop_filt.append(pop)\n",
    "    return pop_filt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T06:13:26.961789Z",
     "start_time": "2021-07-20T06:13:21.821449Z"
    }
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import sys\n",
    "\n",
    "# Args\n",
    "version = '2021.07.19'\n",
    "\n",
    "# Gnomad hail table:\n",
    "hail_table = 'gs://gcp-public-data--gnomad/release/3.1.1/ht/genomes/gnomad.genomes.v3.1.1.sites.ht'\n",
    "\n",
    "# Grch38 to 37 chainfile:\n",
    "chain_file = 'gs://hail-common/references/grch38_to_grch37.over.chain.gz'\n",
    "\n",
    "# Output \n",
    "out_parquet = f'gs://ot-team/dsuveges/variant_table/{version}/variant-annotation.parquet'\n",
    "out_sitelist = f'gs://ot-team/dsuveges/variant_table/{version}//variant-annotation.sitelist.tsv.gz'\n",
    "\n",
    "out_partitions = 256\n",
    "maf_threshold = 0.001 # 0.1%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-20T06:23:09.639115Z",
     "start_time": "2021-07-20T06:19:36.017629Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FatalError",
     "evalue": "SocketTimeoutException: connect timed out\n\nJava stack trace:\njava.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:246)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:462)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1443)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1510)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164)\n\tat is.hail.io.fs.FS.isDir(FS.scala:175)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:173)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.net.SocketTimeoutException: connect timed out\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:589)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:211)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:308)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:326)\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032)\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:148)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:184)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:494)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:243)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:462)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1443)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1510)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164)\n\tat is.hail.io.fs.FS.isDir(FS.scala:175)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:173)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n\n\nHail version: 0.2.72-cfce5e858cab\nError summary: SocketTimeoutException: connect timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6a1840115496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhail_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total number of rows: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate smaller dataset:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Variants pre-filtering: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-1461>\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(path, _intervals, _filter_intervals, _n_partitions)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/hail/methods/impex.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(path, _intervals, _filter_intervals, _n_partitions)\u001b[0m\n\u001b[1;32m   2455\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m     \"\"\"\n\u001b[0;32m-> 2457\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrg_config\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_references_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2458\u001b[0m         \u001b[0mhl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReferenceGenome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrg_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mload_references_from_dataset\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_references_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReferenceGenome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromHailDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_fasta_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfasta_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_contigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_contigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmt_contigs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/hail/backend/py4j_backend.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m     31\u001b[0m                                  \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                  'Error summary: %s' % (deepest, full, hail.__version__, deepest), error_id) from None\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: SocketTimeoutException: connect timed out\n\nJava stack trace:\njava.io.IOException: Error getting access token from metadata server at: http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:246)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:462)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1443)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1510)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164)\n\tat is.hail.io.fs.FS.isDir(FS.scala:175)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:173)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n\njava.net.SocketTimeoutException: connect timed out\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:589)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:175)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\n\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:211)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:308)\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:326)\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1202)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1138)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1032)\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:966)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:148)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1012)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory$ComputeCredentialWithRetry.executeRefreshToken(CredentialFactory.java:184)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.api.client.auth.oauth2.Credential.refreshToken(Credential.java:494)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredentialFromMetadataServiceAccount(CredentialFactory.java:243)\n\tat com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.util.CredentialFactory.getCredential(CredentialFactory.java:462)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getCredential(GoogleHadoopFileSystemBase.java:1443)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.createGcsFs(GoogleHadoopFileSystemBase.java:1510)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:1486)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:541)\n\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:494)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164)\n\tat is.hail.io.fs.FS.isDir(FS.scala:175)\n\tat is.hail.io.fs.FS.isDir$(FS.scala:173)\n\tat is.hail.io.fs.HadoopFS.isDir(HadoopFS.scala:70)\n\tat is.hail.expr.ir.RelationalSpec$.readMetadata(AbstractMatrixTableSpec.scala:30)\n\tat is.hail.expr.ir.RelationalSpec$.readReferences(AbstractMatrixTableSpec.scala:68)\n\tat is.hail.variant.ReferenceGenome$.fromHailDataset(ReferenceGenome.scala:596)\n\tat is.hail.variant.ReferenceGenome.fromHailDataset(ReferenceGenome.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n\n\nHail version: 0.2.72-cfce5e858cab\nError summary: SocketTimeoutException: connect timed out"
     ]
    }
   ],
   "source": [
    "ht = hl.read_table(hail_table).head(10_000)\n",
    "print('Total number of rows: ', ht.count())\n",
    "\n",
    "# Generate smaller dataset:\n",
    "print('Variants pre-filtering: ', ht.count())\n",
    "ht = ht.filter(ht.filters.length() == 0)\n",
    "print('Variants post-quality filter: ', ht.count())\n",
    "\n",
    "\n",
    "# Population of interest:\n",
    "populations = {\n",
    "    'afr-adj', \n",
    "    'amr-adj', \n",
    "    'ami-adj', \n",
    "    'asj-adj', \n",
    "    'eas-adj', \n",
    "    'fin-adj', \n",
    "    'nfe-adj', \n",
    "    'mid-adj', \n",
    "    'sas-adj', \n",
    "    'oth-adj'\n",
    "}\n",
    "af_to_maf = lambda af: hl.if_else(af <= 0.5, af, 1 - af)\n",
    "population_indices = ht.globals.freq_index_dict.collect()[0]\n",
    "population_indices = {pop: index for pop,index in population_indices.items() if pop in populations}\n",
    "\n",
    "# Adding population allele frequency and minor allele frequency:\n",
    "ht = ht.annotate(\n",
    "    # Generate struct for alt. allele frequency in selected populations:\n",
    "    af = hl.struct(**{pop: ht.freq[index].AF for pop, index in population_indices.items()}),\n",
    "    \n",
    "    # Generate struct for minor allele frequency for selected populations:\n",
    "    maf = hl.struct(**{pop: af_to_maf(ht.freq[index].AF) for pop, index in population_indices.items()}),\n",
    "    \n",
    "    # Generate an _array_ with maf values for further filtering:\n",
    "    maf_values = hl.array([af_to_maf(ht.freq[index].AF) for pop, index in population_indices.items()])\n",
    ")\n",
    "\n",
    "# Applying maf threshold:\n",
    "ht = ht.filter(hl.max(ht.maf_values) > maf_threshold)\n",
    "print(f'Number of variants after applying MAF filter: {ht.count()}')\n",
    "\n",
    "# # These liftover issues are not yet fully convincing:\n",
    "grch37 = hl.get_reference('GRCh37')\n",
    "grch38 = hl.get_reference('GRCh38')\n",
    "grch38.add_liftover(chain_file, grch37)\n",
    "\n",
    "# # Liftover\n",
    "# ht = ht.annotate(\n",
    "#     locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Variants pre-filtering:  100000\n",
    "Variants post-quality filter:  25468=========================>    (13 + 1) / 14]\n",
    "```\n",
    "\n",
    "The 100_000 variant is dropped to 25k after applying filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-19T14:32:59.433664Z",
     "start_time": "2021-07-19T14:32:59.429430Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-6e3b7cb13571>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-6e3b7cb13571>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    get_max_maf = lambda mafs: True for pop in populations if mafs[pop] > 0.01 else False\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "grch37 = hl.get_reference('GRCh37')  # doctest: +SKIP\n",
    "grch38 = hl.get_reference('GRCh38')  # doctest: +SKIP\n",
    "grch38.add_liftover(chain_file, grch37)  # doctest: +SKIP\n",
    "\n",
    "# Liftover\n",
    "ht = ht.annotate(\n",
    "    locus_GRCh37 = hl.liftover(ht.locus, 'GRCh37')\n",
    ")\n",
    "\n",
    "# Adding build specific coordinates to the table:\n",
    "ht = ht.annotate(\n",
    "    chrom_b38 = ht.locus.contig,\n",
    "    pos_b38 = ht.locus.position,\n",
    "#     chrom_b37 = ht.locus_GRCh37.contig.replace('chr', ''),\n",
    "#     pos_b37 = ht.locus_GRCh37.position,\n",
    "    ref = ht.alleles[0],\n",
    "    alt = ht.alleles[1]\n",
    ")\n",
    "\n",
    "# Selecting relevant column\n",
    "ht = ht.annotate(\n",
    "    cadd = ht.cadd.rename({'raw_score': 'raw'}).drop('has_duplicate')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all globals\n",
    "ht = ht.select_globals()\n",
    "\n",
    "# Drop unnecessary VEP fields\n",
    "ht = ht.annotate(\n",
    "    vep = ht.vep.drop(\n",
    "        'assembly_name', # OK\n",
    "        'allele_string',\n",
    "        'ancestral',\n",
    "#         'colocated_variants', # <- not OK\n",
    "        'context',\n",
    "        'end',\n",
    "        'id',\n",
    "        'input',\n",
    "        'intergenic_consequences',\n",
    "        'seq_region_name',\n",
    "        'start',\n",
    "        'strand',\n",
    "        'variant_class'\n",
    "    ),\n",
    "    locus_GRCh38 = ht.locus\n",
    ")\n",
    "\n",
    "# Sort columns\n",
    "col_order = ['locus_GRCh38', \n",
    "#              'chrom_b37', 'pos_b37', \n",
    "             'chrom_b38', 'pos_b38',\n",
    "             'ref', 'alt', 'allele_info', 'vep', 'rsid', 'af', 'cadd']\n",
    "ht = ht.select(*col_order)\n",
    "\n",
    "# Persist as writing twice would cause re-computation\n",
    "ht = ht.persist()\n",
    "\n",
    "# Repartition and write parquet file\n",
    "(\n",
    "    ht.to_spark(flatten=False)\n",
    "      .repartition(out_partitions)\n",
    "      .write.parquet(out_parquet)\n",
    ")\n",
    "\n",
    "# Export site list\n",
    "cols = ['chrom_b37', 'pos_b37', 'chrom_b38', 'pos_b38', 'ref', 'alt', 'rsid']\n",
    "(\n",
    "    ht.select(*cols)\n",
    "      .export(out_sitelist)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ht.to_spark(flatten=False)\n",
    "      .repartition(out_partitions)\n",
    "      .write.json(out_parquet.replace('parquet', 'json'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T08:32:41.207952Z",
     "start_time": "2021-07-22T08:32:41.203408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': 'Valami'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_string = '{\"header\": \"Ez valami header\", \"body\": \"Valami\", \"footer\": \"Footer, amit eldobunk\"}'\n",
    "data = json.loads(json_string)\n",
    "\n",
    "for dropped_key in ['footer', 'header']:\n",
    "    if dropped_key in data: del data[dropped_key]\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T09:48:31.405049Z",
     "start_time": "2021-07-26T09:47:52.010710Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-abbb7bd9bdf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-abbb7bd9bdf6>\u001b[0m in \u001b[0;36mreceive_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_INET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPORT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPACKET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeserialize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    " \n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 9999\n",
    "PACKET_SIZE = 110000\n",
    " \n",
    "class Client():\n",
    " \n",
    "    def __init__(self):\n",
    "        self.raw_data = None\n",
    "        self.data = None\n",
    " \n",
    "    def receive_data(self):\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as x:\n",
    "            x.connect((HOST, PORT))\n",
    "            self.raw_data = x.recv(PACKET_SIZE).strip()\n",
    " \n",
    "    def deserialize_data(self):\n",
    "        self.data = json.loads(self.raw_data[\"data\"][\"status\"])\n",
    "\n",
    "client = Client()\n",
    " \n",
    "while True:\n",
    "    time.sleep(2)\n",
    "    client.receive_data()\n",
    "    client.deserialize_data()\n",
    "    print(client.data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-26T14:49:18.208419Z",
     "start_time": "2021-07-26T13:49:17.930581Z"
    },
    "code_folding": []
   },
   "source": [
    "import json\n",
    "import socket\n",
    "import time\n",
    "\n",
    "class Client():\n",
    "\n",
    "    HOST = \"127.0.0.1\"\n",
    "    PORT = 30001\n",
    "    PACKET_SIZE = 110000\n",
    "\n",
    "    def __init__(self, host: str = None, port: int = None, packet_size: int = None) -> None:\n",
    "\n",
    "        self.host = host if host else self.HOST\n",
    "        self.port = port if port else self.PORT\n",
    "        self.packet_size = packet_size if packet_size else self.PACKET_SIZE\n",
    "\n",
    "        try:\n",
    "            # Creating socket:\n",
    "            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            \n",
    "            # Connect:\n",
    "            self.socket.connect((self.host, self.port))\n",
    "            \n",
    "        except ConnectionRefusedError:\n",
    "            raise ConnectionRefusedError(f'Could not connect to {self.host} on port {self.port}')\n",
    "            \n",
    "        print('Connection was successful!')\n",
    "\n",
    "    def close_socket(self) -> None:\n",
    "        self.socket.close()\n",
    "\n",
    "    def get_status(self) -> str:\n",
    "        response = self.socket.recv(self.PACKET_SIZE)\n",
    "        response = response.strip()\n",
    "\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "        except JSONDecodeError:\n",
    "            raise JSONDecodeError(f'data could not be parsed as json: {data}')\n",
    "\n",
    "        try:\n",
    "            status = data['data']['status']\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"keys: ['data']['status'] were not found in data: {data}\")\n",
    "\n",
    "        return status\n",
    "\n",
    "client = Client(host=socket.gethostname(), port=9999)\n",
    "\n",
    "while True:\n",
    "    time.sleep(2)\n",
    "    print(client.get_status())\n",
    "\n",
    "client.close_socket()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
