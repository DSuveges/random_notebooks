{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T14:01:40.504701Z",
     "start_time": "2021-03-16T14:00:24.917755Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://ot-snapshots/link-2/output/cooccurrences/part-00000-0883439b-5aa1-4062-a96f-fc871cb218b5-c000.snappy.parquet...\n",
      "/ [0 files][    0.0 B/231.4 MiB]                                                \r",
      "==> NOTE: You are downloading one or more large file(s), which would\n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "-\r",
      "- [0 files][  2.8 MiB/231.4 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][  6.4 MiB/231.4 MiB]                                                \r",
      "/\r",
      "/ [0 files][ 10.0 MiB/231.4 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 13.7 MiB/231.4 MiB]                                                \r",
      "|\r",
      "| [0 files][ 17.3 MiB/231.4 MiB]                                                \r",
      "/\r",
      "-\r",
      "- [0 files][ 20.4 MiB/231.4 MiB]                                                \r",
      "\\\r",
      "\\ [0 files][ 24.0 MiB/231.4 MiB]                                                \r",
      "|\r",
      "/\r",
      "/ [0 files][ 27.6 MiB/231.4 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 31.2 MiB/231.4 MiB]                                                \r",
      "|\r",
      "| [0 files][ 34.8 MiB/231.4 MiB]                                                \r",
      "/\r",
      "-\r",
      "- [0 files][ 37.9 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "\\ [0 files][ 41.5 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][ 45.1 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "- [0 files][ 48.2 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][ 50.3 MiB/231.4 MiB]    3.0 MiB/s                                   \r",
      "/\r",
      "/ [0 files][ 53.9 MiB/231.4 MiB]    3.1 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 57.5 MiB/231.4 MiB]    3.1 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][ 61.1 MiB/231.4 MiB]    3.1 MiB/s                                   \r",
      "-\r",
      "- [0 files][ 64.7 MiB/231.4 MiB]    3.2 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][ 68.3 MiB/231.4 MiB]    3.5 MiB/s                                   \r",
      "/\r",
      "/ [0 files][ 71.4 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 75.0 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "| [0 files][ 78.6 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][ 82.2 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][ 85.3 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "/ [0 files][ 88.4 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 92.0 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "|\r",
      "| [0 files][ 95.6 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][ 99.3 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][102.9 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "/ [0 files][106.0 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][109.6 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "| [0 files][113.2 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][116.8 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "\\ [0 files][120.4 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][123.5 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][127.1 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "| [0 files][130.7 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][134.3 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "\\ [0 files][137.9 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][141.0 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "- [0 files][144.6 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][148.2 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][151.8 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "\\ [0 files][155.5 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][158.6 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "- [0 files][162.2 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][165.8 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "/ [0 files][169.4 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][173.0 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][176.1 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "- [0 files][179.7 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][182.8 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][186.4 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "\\\r",
      "\\ [0 files][190.0 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][193.1 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "-\r",
      "- [0 files][196.7 MiB/231.4 MiB]    3.3 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][200.1 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "/\r",
      "/ [0 files][203.7 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][207.3 MiB/231.4 MiB]    3.4 MiB/s                                   \r",
      "|\r",
      "| [0 files][209.9 MiB/231.4 MiB]    3.2 MiB/s                                   \r",
      "/\r",
      "-\r",
      "- [0 files][211.7 MiB/231.4 MiB]    2.8 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][215.3 MiB/231.4 MiB]    2.8 MiB/s                                   \r",
      "/\r",
      "/ [0 files][218.6 MiB/231.4 MiB]    2.8 MiB/s                                   \r",
      "-\r",
      "\\\r",
      "\\ [0 files][222.2 MiB/231.4 MiB]    2.8 MiB/s                                   \r",
      "|\r",
      "/\r",
      "/ [0 files][225.8 MiB/231.4 MiB]    3.2 MiB/s                                   \r",
      "-\r",
      "- [0 files][228.7 MiB/231.4 MiB]    3.1 MiB/s                                   \r",
      "\\\r",
      "|\r",
      "| [0 files][229.4 MiB/231.4 MiB]    2.3 MiB/s                                   \r",
      "/\r",
      "/ [0 files][230.0 MiB/231.4 MiB]    1.5 MiB/s                                   \r",
      "-\r",
      "- [1 files][231.4 MiB/231.4 MiB]    1.2 MiB/s                                   \r\n",
      "Operation completed over 1 objects/231.4 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil cp -r gs://ot-snapshots/link-2/output/cooccurrences/part-00000-0883439b-5aa1-4062-a96f-fc871cb218b5-c000.snappy.parquet \\\n",
    "    /Users/dsuveges/project_data/epmc_evidence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T22:13:32.546945Z",
     "start_time": "2021-03-18T22:12:50.637472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3e46d340a023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m cooc_df = (\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepmc_cooc_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"GP-DS\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'isMapped'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pmid'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionKeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "\n",
    "global spark\n",
    "\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '20g')\n",
    "\n",
    "spark = (pyspark.sql.SparkSession\n",
    "    .builder\n",
    "    .appName(\"phenodigm_parser\")\n",
    "    .config(\"spark.executor.memory\", '10g')\n",
    "     .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "    .config(\"spark.driver.memory\", '10g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#   \n",
    "\n",
    "partitionKeys = ['pmid', 'targetFromSourceId', 'diseaseFromSourceMappedId']\n",
    "print('Spark version: ', spark.version)\n",
    "epmc_cooc_file = '/Users/dsuveges/project_data/epmc_evidence/cooccurrences/*'\n",
    "# cooc_df = (\n",
    "#     spark.read.parquet(epmc_cooc_file)\n",
    "#     .filter((col('type') == \"GP-DS\") & (col('isMapped') == True) & (col('pmid') != \"\"))\n",
    "#     .withColumnRenamed(\"keywordId1\", \"targetFromSourceId\")\n",
    "#     .withColumnRenamed(\"keywordId2\", \"diseaseFromSourceMappedId\")\n",
    "#     .withColumnRenamed(\"label1\", \"targetFromSource\")\n",
    "#     .withColumnRenamed(\"label2\", \"diseaseFromSource\")\n",
    "#     .dropDuplicates(partitionKeys)\n",
    "#     .filter(col('targetFromSource') == '(')\n",
    "#     .select('pmid', 'targetFromSource', 'targetFromSourceId', 'diseaseFromSourceMappedId', 'section', 'evidence_score')\n",
    "#     .toPandas()\n",
    "# )\n",
    "\n",
    "partitionKeys = ['pmid', 'keywordId1', 'keywordId2']\n",
    "\n",
    "cooc_df = (\n",
    "    spark.read.json(epmc_cooc_file)\n",
    "    .filter((col('type') == \"GP-DS\") & (col('isMapped') == True) & (col('pmid') != \"\") & (col('label1') == '('))\n",
    "    .dropDuplicates(partitionKeys)\n",
    "    .write.json('weird_names.json')\n",
    ")\n",
    "\n",
    "\n",
    "# 'pmid',\n",
    "#  'pubDate',\n",
    "#  'organisms',\n",
    "#  'section',\n",
    "#  'text',\n",
    "#  'association',\n",
    "#  'end1',\n",
    "#  'end2',\n",
    "#  'evidence_score',\n",
    "#  'targetFromSource',\n",
    "#  'targetFromSourceId',\n",
    "#  'diseaseFromSource',\n",
    "#  'diseaseFromSourceMappedId',\n",
    "#  'relation',\n",
    "#  'start1',\n",
    "#  'start2',\n",
    "#  'type',\n",
    "#  'type1',\n",
    "#  'type2',\n",
    "#  'isMapped'\n",
    "print(len(cooc_df))\n",
    "cooc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T22:28:50.491825Z",
     "start_time": "2021-03-18T22:19:22.540178Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o151.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:956)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:954)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2221)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2134)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1967)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1967)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 32 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d5a208c8b1fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Save output:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o151.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:956)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:954)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:954)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2221)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2134)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1967)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1967)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 32 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import re\n",
    "\n",
    "global spark\n",
    "\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '20g')\n",
    "\n",
    "sparkConf = (SparkConf()\n",
    "         .set(\"spark.driver.memory\", \"15g\")\n",
    "         .set(\"spark.executor.memory\", \"15g\")\n",
    "         .set(\"spark.driver.maxResultSize\", \"0\")\n",
    "         .set(\"spark.debug.maxToStringFields\", \"2000\")\n",
    "         .set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"500000\")\n",
    "         )\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .config(conf=sparkConf)\n",
    "         .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "        .master('local[*]')\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "cooccurrenceFile = '/Users/dsuveges/project_data/epmc_evidence/cooccurrences/*'\n",
    "outputFile = '2021.03.18_epmc.json.gz'\n",
    "\n",
    "partitionKeys = ['pmid', 'targetFromSourceId', 'diseaseFromSourceMappedId']\n",
    "w = Window.partitionBy(*partitionKeys)\n",
    "(\n",
    "    # Reading file:\n",
    "    spark.read.json(cooccurrenceFile)\n",
    "\n",
    "    # Filtering for diases/target cooccurrences:\n",
    "    .filter((col('type') == \"GP-DS\") & (col('isMapped') == True) & (col('pmid') != \"\"))\n",
    "\n",
    "    # Renaming columns:\n",
    "    .withColumnRenamed(\"keywordId1\", \"targetFromSourceId\")\n",
    "    .withColumnRenamed(\"keywordId2\", \"diseaseFromSourceMappedId\")\n",
    "    .withColumnRenamed(\"label1\", \"targetFromSource\")\n",
    "    .withColumnRenamed(\"label2\", \"diseaseFromSource\")\n",
    "\n",
    "        # collect sets of field values per window aggregation in w with keys partitionKeys\n",
    "    .withColumn('textMiningSentences', collect_set(\n",
    "            struct(\n",
    "                col(\"text\"),\n",
    "                col('start1').alias('tStart'),\n",
    "                col(\"end1\").alias('tEnd'),\n",
    "                col('start2').alias('dStart'),\n",
    "                col(\"end2\").alias('dEnd'),\n",
    "                col('section')\n",
    "            )).over(w)\n",
    "        )\n",
    "    .withColumn('resourceScore', sum(col('evidence_score')).over(w))\n",
    "    .filter(col('resourceScore') >= 2)\n",
    "    .dropDuplicates(partitionKeys)\n",
    "    .withColumn(\"literature\", array(col('pmid')))\n",
    "\n",
    "    # Adding linteral columns:\n",
    "    .withColumn('datasourceId',lit('europepmc'))\n",
    "    .withColumn('datatypeId',lit('literature'))\n",
    "\n",
    "    # Reorder columns:\n",
    "    .select([\"datasourceId\", \"datatypeId\", \"targetFromSource\", \"targetFromSourceId\",'resourceScore',\n",
    "            \"diseaseFromSource\",\"diseaseFromSourceMappedId\",\"literature\",\"textMiningSentences\"])\n",
    "\n",
    "    # Save output:\n",
    "    .write.format('json').mode('overwrite').option('compression', 'gzip').save(outputFile)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:25:38.834576Z",
     "start_time": "2021-03-17T14:20:32.933818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "   target         disease     paper  score first_section  \\\n",
      "0  O14813     EFO_1001985  11882252   15.2      abstract   \n",
      "1  O14813  Orphanet_45358  11882252   12.2         title   \n",
      "2  P01108     EFO_0000621  11922865    8.0         other   \n",
      "3  P01138     EFO_0000239  11922865    8.4      abstract   \n",
      "4  P01138     EFO_0000621  11922865    6.4         other   \n",
      "\n",
      "                                            sections  \n",
      "0  [abstract, abstract, abstract, other, other, o...  \n",
      "1                                     [title, other]  \n",
      "2  [other, other, other, other, other, other, oth...  \n",
      "3                                  [abstract, other]  \n",
      "4                                     [other, other]  \n"
     ]
    }
   ],
   "source": [
    "# old_file = 'old_epmc_sample.json'\n",
    "old_file = '/Users/dsuveges/project_data/ot/evidence_input/21.02/epmc/cttv025-25-01-2021.json.gz'\n",
    "# cooc_df = (\n",
    "#     spark.read.json(old_file)\n",
    "#     .filter(size(col('evidence.literature_ref.mined_sentences')) == 1)\n",
    "#     .withColumn(\"sentences\", explode(col('evidence.literature_ref.mined_sentences')))\n",
    "#     .select(col('unique_association_fields.target_id').alias('target_id'), \n",
    "#         col('unique_association_fields.publication_id').alias('publication_id'), \n",
    "#         col('unique_association_fields.disease_id').alias('disease_id'), \n",
    "#         col('evidence.resource_score.value').alias('score'),\n",
    "#         col('evidence.literature_ref.mined_sentences').alias('sentences'),\n",
    "#         col('sentences.section').alias('section'))\n",
    "#     .toPandas()\n",
    "# )\n",
    "\n",
    "# cooc_df.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "old_data = []\n",
    "\n",
    "with gzip.open(old_file) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "#         if len(data['evidence']['literature_ref']['mined_sentences']) > 1:\n",
    "#             continue \n",
    "            \n",
    "        old_data.append({\n",
    "            'target': data['target']['id'].split('/')[-1],\n",
    "            'disease': data['disease']['id'].split('/')[-1],\n",
    "            'paper': data['evidence']['unique_experiment_reference'].split('/')[-1],\n",
    "            'score': data['evidence']['resource_score']['value'],\n",
    "            'first_section': data['evidence']['literature_ref']['mined_sentences'][0]['section'],\n",
    "            'sections': [x['section'] for x in data['evidence']['literature_ref']['mined_sentences']]\n",
    "        })\n",
    "\n",
    "old_df = pd.DataFrame(old_data)\n",
    "print(f'Number of evidence: {len(old_df)}')\n",
    "print(old_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:29:33.194737Z",
     "start_time": "2021-03-17T14:29:33.178360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7976568"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:30:10.562034Z",
     "start_time": "2021-03-17T14:30:07.105986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "abstract    5629820\n",
       "title       1417215\n",
       "other        817758\n",
       "figure        64473\n",
       "table         46586\n",
       "appendix        716\n",
       "Name: first_section, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_df.first_section.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:38:40.064022Z",
     "start_time": "2021-03-17T14:38:39.955466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>disease</th>\n",
       "      <th>paper</th>\n",
       "      <th>score</th>\n",
       "      <th>first_section</th>\n",
       "      <th>sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>Q9H293</td>\n",
       "      <td>EFO_0005741</td>\n",
       "      <td>16606667</td>\n",
       "      <td>107.8</td>\n",
       "      <td>abstract</td>\n",
       "      <td>[abstract, abstract, figure, figure, figure, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>Q8N474</td>\n",
       "      <td>EFO_0000182</td>\n",
       "      <td>17626620</td>\n",
       "      <td>149.0</td>\n",
       "      <td>title</td>\n",
       "      <td>[title, abstract, abstract, abstract, abstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15221</th>\n",
       "      <td>P29590</td>\n",
       "      <td>Orphanet_647</td>\n",
       "      <td>17030982</td>\n",
       "      <td>193.8</td>\n",
       "      <td>abstract</td>\n",
       "      <td>[abstract, abstract, figure, figure, figure, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20021</th>\n",
       "      <td>P00519</td>\n",
       "      <td>EFO_0004289</td>\n",
       "      <td>15939795</td>\n",
       "      <td>135.6</td>\n",
       "      <td>abstract</td>\n",
       "      <td>[abstract, figure, figure, figure, figure, fig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20030</th>\n",
       "      <td>P11274</td>\n",
       "      <td>EFO_0004289</td>\n",
       "      <td>15939795</td>\n",
       "      <td>135.6</td>\n",
       "      <td>abstract</td>\n",
       "      <td>[abstract, figure, figure, figure, figure, fig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431574</th>\n",
       "      <td>P23470</td>\n",
       "      <td>EFO_0003060</td>\n",
       "      <td>33174523</td>\n",
       "      <td>115.0</td>\n",
       "      <td>title</td>\n",
       "      <td>[title, abstract, abstract, abstract, abstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432104</th>\n",
       "      <td>Q11201</td>\n",
       "      <td>EFO_0000756</td>\n",
       "      <td>33203881</td>\n",
       "      <td>106.0</td>\n",
       "      <td>title</td>\n",
       "      <td>[title, abstract, abstract, abstract, abstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432532</th>\n",
       "      <td>Q9H7M9</td>\n",
       "      <td>MONDO_0007254</td>\n",
       "      <td>33250890</td>\n",
       "      <td>102.6</td>\n",
       "      <td>title</td>\n",
       "      <td>[title, abstract, abstract, abstract, abstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436681</th>\n",
       "      <td>Q8NE86</td>\n",
       "      <td>EFO_0003897</td>\n",
       "      <td>33235465</td>\n",
       "      <td>101.6</td>\n",
       "      <td>abstract</td>\n",
       "      <td>[abstract, abstract, abstract, abstract, abstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437282</th>\n",
       "      <td>P56705</td>\n",
       "      <td>EFO_1001951</td>\n",
       "      <td>33222684</td>\n",
       "      <td>125.8</td>\n",
       "      <td>title</td>\n",
       "      <td>[title, abstract, abstract, abstract, abstract...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1116 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target        disease     paper  score first_section  \\\n",
       "7928     Q9H293    EFO_0005741  16606667  107.8      abstract   \n",
       "10125    Q8N474    EFO_0000182  17626620  149.0         title   \n",
       "15221    P29590   Orphanet_647  17030982  193.8      abstract   \n",
       "20021    P00519    EFO_0004289  15939795  135.6      abstract   \n",
       "20030    P11274    EFO_0004289  15939795  135.6      abstract   \n",
       "...         ...            ...       ...    ...           ...   \n",
       "1431574  P23470    EFO_0003060  33174523  115.0         title   \n",
       "1432104  Q11201    EFO_0000756  33203881  106.0         title   \n",
       "1432532  Q9H7M9  MONDO_0007254  33250890  102.6         title   \n",
       "1436681  Q8NE86    EFO_0003897  33235465  101.6      abstract   \n",
       "1437282  P56705    EFO_1001951  33222684  125.8         title   \n",
       "\n",
       "                                                  sections  \n",
       "7928     [abstract, abstract, figure, figure, figure, f...  \n",
       "10125    [title, abstract, abstract, abstract, abstract...  \n",
       "15221    [abstract, abstract, figure, figure, figure, f...  \n",
       "20021    [abstract, figure, figure, figure, figure, fig...  \n",
       "20030    [abstract, figure, figure, figure, figure, fig...  \n",
       "...                                                    ...  \n",
       "1431574  [title, abstract, abstract, abstract, abstract...  \n",
       "1432104  [title, abstract, abstract, abstract, abstract...  \n",
       "1432532  [title, abstract, abstract, abstract, abstract...  \n",
       "1436681  [abstract, abstract, abstract, abstract, abstr...  \n",
       "1437282  [title, abstract, abstract, abstract, abstract...  \n",
       "\n",
       "[1116 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_df.loc[old_df.score > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T14:47:56.816085Z",
     "start_time": "2021-03-17T14:47:55.958430Z"
    }
   },
   "outputs": [],
   "source": [
    "filtered = old_df.loc[old_df.paper == '33203881',['target', 'disease', 'score', 'paper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T13:28:31.933589Z",
     "start_time": "2021-03-25T13:28:26.819815Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "\n",
    "global spark\n",
    "\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '20g')\n",
    "\n",
    "spark = (pyspark.sql.SparkSession\n",
    "    .builder\n",
    "    .appName(\"phenodigm_parser\")\n",
    "    .config(\"spark.executor.memory\", '10g')\n",
    "     .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "    .config(\"spark.driver.memory\", '10g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-24T16:30:20.965438Z",
     "start_time": "2021-03-24T16:23:13.976727Z"
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.json('/Users/dsuveges/project_data/ot/evidence_input/21.04/phenodigm-2021-03-24.json.gz')\n",
    "    .withColumn(\"biologicalModelId\", \n",
    "                when(col(\"biologicalModelId\").startswith(\"MGI\"), col(\"biologicalModelId\")).otherwise(None))\n",
    "    .write.format('json').mode('overwrite').option('compression', 'gzip').save('/Users/dsuveges/project_data/ot/evidence_input/21.04/phenodigm-2021-03-24_b.json.gz')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T14:05:30.515013Z",
     "start_time": "2021-03-25T14:05:12.187089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasourceId: string (nullable = true)\n",
      " |-- datatypeId: string (nullable = true)\n",
      " |-- diseaseFromSource: string (nullable = true)\n",
      " |-- diseaseFromSourceMappedId: string (nullable = true)\n",
      " |-- literature: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- resourceScore: double (nullable = true)\n",
      " |-- targetFromSource: string (nullable = true)\n",
      " |-- targetFromSourceId: string (nullable = true)\n",
      " |-- textMiningSentences: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- dEnd: long (nullable = true)\n",
      " |    |    |-- dStart: long (nullable = true)\n",
      " |    |    |-- section: string (nullable = true)\n",
      " |    |    |-- tEnd: long (nullable = true)\n",
      " |    |    |-- tStart: long (nullable = true)\n",
      " |    |    |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    spark.read.json('/Users/dsuveges/Downloads/part-00000-2aa22b2a-5c07-48ae-919d-9cc14eaa7da6-c000.json.gz')\n",
    "    .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T13:31:15.003185Z",
     "start_time": "2021-03-25T13:31:14.829255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|                 col|targetFromSourceId|   diseaseFromSource|\n",
      "+--------------------+------------------+--------------------+\n",
      "|[, 108, 95, abstr...|   ENSG00000112715|      bladder tumors|\n",
      "|[, 49, 35, abstra...|   ENSG00000112715|      bladder tumors|\n",
      "|[, 83, 69, abstra...|   ENSG00000112715|      bladder tumors|\n",
      "|[, 85, 69, title,...|   ENSG00000136997|    Burkitt lymphoma|\n",
      "|[, 104, 88, abstr...|   ENSG00000136997|    Burkitt lymphoma|\n",
      "|[, 183, 171, abst...|   ENSG00000118702|        hypoglycemia|\n",
      "|[, 352, 349, abst...|   ENSG00000123384|acute myeloid leu...|\n",
      "|[, 352, 349, abst...|   ENSG00000123384|acute myeloid leu...|\n",
      "|[, 136, 114, titl...|   ENSG00000123384|acute myeloid leu...|\n",
      "|[, 146, 143, abst...|   ENSG00000123384|acute myeloid leu...|\n",
      "|[, 133, 130, abst...|   ENSG00000123384|acute myeloid leu...|\n",
      "|[, 277, 274, abst...|   ENSG00000113161|                 CAD|\n",
      "|[, 155, 144, abst...|   ENSG00000134184|         lung cancer|\n",
      "|[, 146, 135, abst...|   ENSG00000134184|         lung cancer|\n",
      "|[, 77, 66, abstra...|   ENSG00000134184|         lung cancer|\n",
      "|[, 146, 135, abst...|   ENSG00000134184|         lung cancer|\n",
      "|[, 113, 101, abst...|   ENSG00000156006|        colon cancer|\n",
      "|[, 89, 77, abstra...|   ENSG00000112715|        glioblastoma|\n",
      "|[, 129, 106, abst...|   ENSG00000143839|end-stage renal f...|\n",
      "|[, 71, 58, abstra...|   ENSG00000057593|       liver disease|\n",
      "+--------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T19:37:44.494100Z",
     "start_time": "2021-03-26T19:37:43.810181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- pubDate: string (nullable = true)\n",
      " |-- organisms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- section: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- association: struct (nullable = true)\n",
      " |    |-- Altered_Expression: double (nullable = true)\n",
      " |    |-- Any: double (nullable = true)\n",
      " |    |-- Genetic_Variation: double (nullable = true)\n",
      " |    |-- Negative: double (nullable = true)\n",
      " |    |-- Neutral: double (nullable = true)\n",
      " |    |-- No: double (nullable = true)\n",
      " |    |-- Positive: double (nullable = true)\n",
      " |    |-- Regulatory_modification: double (nullable = true)\n",
      " |    |-- Yes: double (nullable = true)\n",
      " |-- end1: long (nullable = true)\n",
      " |-- end2: long (nullable = true)\n",
      " |-- evidence_score: double (nullable = true)\n",
      " |-- label1: string (nullable = true)\n",
      " |-- keywordId1: string (nullable = true)\n",
      " |-- label2: string (nullable = true)\n",
      " |-- keywordId2: string (nullable = true)\n",
      " |-- relation: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- endr: long (nullable = true)\n",
      " |    |    |-- labelr: string (nullable = true)\n",
      " |    |    |-- startr: long (nullable = true)\n",
      " |    |    |-- typer: string (nullable = true)\n",
      " |-- start1: long (nullable = true)\n",
      " |-- start2: long (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- type1: string (nullable = true)\n",
      " |-- type2: string (nullable = true)\n",
      " |-- isMapped: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (\n",
    "#     spark.read.parquet('/Users/dsuveges/project_data/epmc_evidence/raw_epmc_cooc/part-00007-23c5c03e-7e4c-4e79-92da-e9bcb3eef403-c000.snappy.parquet')\n",
    "#     .printSchema()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T19:46:09.426532Z",
     "start_time": "2021-03-26T19:46:06.482612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "(\n",
    "    spark.read.parquet('/Users/dsuveges/project_data/epmc_evidence/raw_epmc_cooc/part-00007-23c5c03e-7e4c-4e79-92da-e9bcb3eef403-c000.snappy.parquet')\n",
    "    .filter(\n",
    "        (col('type') == \"GP-DS\") & # Filtering for gene/disease cooccurrence\n",
    "        (col('isMapped') == True) & # Filtering for mapping\n",
    "        (col('pmid') != \"\") &  # Filtering out missing pmids\n",
    "        (col('label1') != \"(\") & # Filtering out this strange entity.\n",
    "        (length(col('text')) > 500 )) # Sentence threshold is 500 \n",
    "    .withColumn('text_lenght', length(col('text')))\n",
    "    .select(col('text_lenght'),col('text'))\n",
    "    .dropDuplicates()\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T20:19:22.461311Z",
     "start_time": "2021-03-26T20:19:19.860838Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark.read.parquet('/Users/dsuveges/project_data/epmc_evidence/raw_epmc_cooc/part-00007-23c5c03e-7e4c-4e79-92da-e9bcb3eef403-c000.snappy.parquet')\n",
    "    .filter(\n",
    "        (col('type') == \"GP-DS\") & # Filtering for gene/disease cooccurrence\n",
    "        (col('isMapped') == True) & # Filtering for mapping\n",
    "        (col('pmid') != \"\") &  # Filtering out missing pmids\n",
    "        (col('label1') != \"(\") & # Filtering out this strange entity.\n",
    "        (length(col('text')) > 500 )) # Sentence threshold is 500 \n",
    "    .groupby([col('text'), col('pmid')])\n",
    "    .agg(\n",
    "        first(col(\"section\")).alias('section'),\n",
    "        length(col('text')).alias('textLenght'),\n",
    "        collect_set(struct(\n",
    "            col('keywordId1').alias('targetFromSourceId'),\n",
    "            col(\"keywordId2\").alias('diseaseFromSourceMappedId'),\n",
    "        )).alias('associations'),\n",
    "        collect_set(struct(\n",
    "            col('label1').alias('targetLabel'),\n",
    "            col(\"label2\").alias('diseaseLabel'),\n",
    "        )).alias('cooccurrences')\n",
    "    )\n",
    "    .withColumn('associationCount', size(col('associations')))\n",
    "    .withColumn('cooccurrenceCount', size(col('cooccurrences')))\n",
    "\n",
    "    .coalesce(1).write.format('json').mode('overwrite').option('compression', 'gzip')\n",
    "    .save('epmc_long_sentences.json.gz')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T16:15:32.096108Z",
     "start_time": "2021-03-27T16:13:00.721119Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('epmc-longSentences-600-2021-03-25.json.gz')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T16:15:32.257355Z",
     "start_time": "2021-03-27T16:15:32.098104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----------------+--------------------+----------+--------------------+--------------------+----------+\n",
      "|associationCount|        associations|cooccurrenceCount|       cooccurrences|      pmid|             section|                text|textLenght|\n",
      "+----------------+--------------------+-----------------+--------------------+----------+--------------------+--------------------+----------+\n",
      "|               6|[[MONDO_0008199, ...|                2|[[PD, Aromatase],...|PMC6993383|characteristics o...|(%) (N = 971)Stud...|      3131|\n",
      "|              12|[[EFO_0000673, EN...|                2|[[prostate cancer...|PMC5681753|             results|(%) (missing = 14...|      1184|\n",
      "|              24|[[EFO_0010133, EN...|               20|[[hypoglycemia, C...|PMC4480895|baseline patient ...|(%) 1,5 AG: 1,5 a...|       892|\n",
      "|               1|[[EFO_0000249, EN...|                1|          [[AD, AÎ²]]|PMC6805864|participant chara...|(%)73 (27.14)25 (...|       668|\n",
      "|              11|[[EFO_0000558, EN...|               12|[[bacterial pneum...|PMC4885647|             results|(%)Cryptococcal m...|      3248|\n",
      "|              16|[[Orphanet_208650...|                2|[[pneumonia, CAP]...|PMC5741879|indication and an...|(%)]175/389a (44....|      1138|\n",
      "|               2|[[EFO_0000222, EN...|                2|[[AML, DKK3], [AL...|PMC5609988|dkk3 and mir-708 ...|(A and D) Compare...|       617|\n",
      "|               6|[[EFO_0000305, EN...|                6|[[invasive ductal...|PMC5438712|plcd1 suppresses ...|(A) Correlation b...|       672|\n",
      "|               1|[[EFO_0003060, EN...|                1|     [[NSCLC, TLR3]]|PMC6776648|tlr3 protein expr...|(A) Kaplan-Meier ...|       946|\n",
      "|               1|[[EFO_0000756, EN...|                1| [[melanoma, CXCR5]]|PMC6170179|single cell trans...|(A) Pearson corre...|       757|\n",
      "|               7|[[EFO_0003060, EN...|                4|[[NSCLC, ALK], [N...|PMC6787520|  secondary outcomes|(A) Pooled HR for...|      1024|\n",
      "|               2|[[MONDO_0024306, ...|                1|[[lactic acidosis...|PMC4178214|redirection of gl...|(A) Protein level...|       846|\n",
      "|               6|[[EFO_0000178, EN...|                1|[[gastric cancer,...|PMC5880597|overall survival ...|(A) Relationship ...|       717|\n",
      "|               4|[[EFO_1001486, EN...|                4|[[PBC, IFNÎ³], [PB...|PMC6874097|low dose il-12 pr...|(A) Representativ...|       668|\n",
      "|               2|[[EFO_0004509, EN...|                3|[[tumor, Yap1], [...|PMC6407673|mice treated with...|(A) Schematic det...|       871|\n",
      "|               2|[[EFO_0005856, EN...|                2|[[arthritis, IL-1...|PMC4060547|increased joint s...|(A) The levels of...|       803|\n",
      "|              10|[[MONDO_0007254, ...|                5|[[breast cancer, ...|PMC5610006|mir-19b promotes ...|(A) The survival ...|       712|\n",
      "|              12|[[EFO_0000621, EN...|                4|[[NB, VEGF], [NB,...|PMC4359454|itln1 facilitates...|(A) Western blot ...|      1097|\n",
      "|               5|[[MONDO_0021063, ...|                1|[[colon cancer, G...|PMC5731866|short overall sur...|(A-B) Kaplan-Meie...|       647|\n",
      "|               4|[[EFO_0000616, EN...|                2|[[tumours, PAR], ...|PMC6498996|resistant tumours...|(B, C) Quantitati...|       672|\n",
      "+----------------+--------------------+-----------------+--------------------+----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T16:27:32.419917Z",
     "start_time": "2021-03-27T16:27:27.912431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, textLenght: string]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slim = (\n",
    "    df\n",
    "    .select(\n",
    "        col('textLenght'),\n",
    "        col('pmid'),\n",
    "        col('associationCount'),\n",
    "        col('cooccurrenceCount')\n",
    "    )\n",
    ")\n",
    "\n",
    "df_slim.describe('textLenght')\n",
    "                 \n",
    "                 \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-27T16:27:51.983165Z",
     "start_time": "2021-03-27T16:27:47.809498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        textLenght|\n",
      "+-------+------------------+\n",
      "|  count|            271215|\n",
      "|   mean|1114.1172575263167|\n",
      "| stddev|1016.5778698172317|\n",
      "|    min|               601|\n",
      "|    max|             90390|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_slim.describe('textLenght').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
