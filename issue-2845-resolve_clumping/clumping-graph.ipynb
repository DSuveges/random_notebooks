{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/01 14:07:53 WARN org.apache.spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '<pwd_output>' appears to be on the local filesystem.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+\n",
      "|studyId|variantId|explainedBy|\n",
      "+-------+---------+-----------+\n",
      "|     s1|       v1|         v1|\n",
      "|     s1|       v2|         v2|\n",
      "|     s1|       v3|         v1|\n",
      "|     s1|       v4|         v1|\n",
      "|     s1|       v5|         v3|\n",
      "|     s1|       v6|         v3|\n",
      "|     s1|       v6|         v2|\n",
      "|     s1|       v7|         v7|\n",
      "|     s1|       v8|         v7|\n",
      "|     s1|       v9|         v5|\n",
      "+-------+---------+-----------+\n",
      "\n",
      "+-------+---+---------+\n",
      "|studyId| id| leadType|\n",
      "+-------+---+---------+\n",
      "|     s1| v3|explained|\n",
      "|     s1| v4|explained|\n",
      "|     s1| v6|explained|\n",
      "|     s1| v2|     root|\n",
      "|     s1| v8|explained|\n",
      "|     s1| v1|     root|\n",
      "|     s1| v7|     root|\n",
      "|     s1| v9|explained|\n",
      "|     s1| v5|explained|\n",
      "+-------+---+---------+\n",
      "\n",
      "+---+---+--------+\n",
      "|src|dst|edgeType|\n",
      "+---+---+--------+\n",
      "| v6| v2|explains|\n",
      "| v9| v5|explains|\n",
      "| v2| v2|explains|\n",
      "| v5| v3|explains|\n",
      "| v8| v7|explains|\n",
      "| v7| v7|explains|\n",
      "| v6| v3|explains|\n",
      "| v4| v1|explains|\n",
      "| v3| v1|explains|\n",
      "| v1| v1|explains|\n",
      "+---+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as f, types as t, Column, DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# GraphFrames needs this:\n",
    "spark.sparkContext.setCheckpointDir('<pwd_output>')\n",
    "\n",
    "\n",
    "data = [\n",
    "    ('s1', 'v1', 'v1'),\n",
    "    ('s1', 'v2', 'v2'),\n",
    "    ('s1', 'v3', 'v1'),\n",
    "    ('s1', 'v4', 'v1'),\n",
    "    ('s1', 'v5', 'v3'),\n",
    "    ('s1', 'v6', 'v3'),\n",
    "    ('s1', 'v6', 'v2'),\n",
    "    ('s1', 'v7', 'v7'),\n",
    "    ('s1', 'v8', 'v7'),\n",
    "    ('s1', 'v9', 'v5'),\n",
    "]\n",
    "\n",
    "colnames = ['studyId', 'variantId', 'explainedBy']\n",
    "\n",
    "df = spark.createDataFrame(data, colnames).persist()\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Convert to vertices:\n",
    "nodes = (\n",
    "    df\n",
    "    .select(\n",
    "        'studyId',\n",
    "        f.col('variantId').alias('id'),\n",
    "        f.when(f.col('variantId') == f.col('explainedBy'), 'root').otherwise('explained').alias('leadType')\n",
    "    )\n",
    "    .distinct()\n",
    "    .persist()\n",
    ")\n",
    "nodes.show()\n",
    "\n",
    "\n",
    "# Convert to edges (more significant points to less significant):\n",
    "edges = (\n",
    "    df\n",
    "    .select(\n",
    "        f.col('variantId').alias('src'),\n",
    "        f.col('explainedBy').alias('dst'),\n",
    "        f.lit('explains').alias('edgeType')\n",
    "    )\n",
    "    .distinct()\n",
    "    .persist()\n",
    ")\n",
    "edges.show()\n",
    "\n",
    "graph = GraphFrame(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------------+\n",
      "|               from|                e0|            to|\n",
      "+-------------------+------------------+--------------+\n",
      "|{s1, v3, explained}|{v3, v1, explains}|{s1, v1, root}|\n",
      "|{s1, v4, explained}|{v4, v1, explains}|{s1, v1, root}|\n",
      "|{s1, v6, explained}|{v6, v2, explains}|{s1, v2, root}|\n",
      "|{s1, v8, explained}|{v8, v7, explains}|{s1, v7, root}|\n",
      "+-------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paths = graph.bfs('leadType == \"explained\"', 'leadType == \"root\"', maxPathLength=30)\n",
    "paths.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/01 14:56:36 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:36 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:36 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:36 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:36 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:37 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:37 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "23/02/01 14:56:37 WARN org.apache.spark.sql.execution.CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------------+-----------+\n",
      "|studyId| id| leadType|   component|explainedBy|\n",
      "+-------+---+---------+------------+-----------+\n",
      "|     s1| v4|explained|309237645312|   [v2, v1]|\n",
      "|     s1| v2|     root|309237645312|   [v2, v1]|\n",
      "|     s1| v5|explained|309237645312|   [v2, v1]|\n",
      "|     s1| v3|explained|309237645312|   [v2, v1]|\n",
      "|     s1| v6|explained|309237645312|   [v2, v1]|\n",
      "|     s1| v1|     root|309237645312|   [v2, v1]|\n",
      "|     s1| v9|explained|309237645312|   [v2, v1]|\n",
      "|     s1| v7|     root|206158430208|       [v7]|\n",
      "|     s1| v8|explained|206158430208|       [v7]|\n",
      "+-------+---+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    graph\n",
    "    .connectedComponents()\n",
    "    .withColumn(\n",
    "        'explainedBy',\n",
    "        f.collect_set(f.when(f.col('leadType') =='root', f.col('id'))).over(Window.partitionBy('component'))\n",
    "    )\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1116:==================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------+--------------+--------------------+---------+--------------------+\n",
      "|   studyId|       variantId|pValueMantissa|pValueExponent|        tagVariantId|R_overall|      qualityControl|\n",
      "+----------+----------------+--------------+--------------+--------------------+---------+--------------------+\n",
      "|GCST004860|10_119029751_G_C|           6.0|            -6|    10_119112913_G_A|-0.789116|[Subsignificant p...|\n",
      "|GCST004860|10_119134504_C_T|           6.0|            -6|    10_119029218_G_A|-0.789984|[Subsignificant p...|\n",
      "|GCST004860|10_119064457_T_C|           2.0|            -7|10_119044943_CTTT...| 0.847645|[Subsignificant p...|\n",
      "|GCST004860| X_107370118_A_G|           2.0|            -7|     X_107363685_T_G| 0.818858|[Subsignificant p...|\n",
      "|GCST004860| X_107370118_A_G|           2.0|            -7|    X_107149542_CA_C| 0.745002|[Subsignificant p...|\n",
      "|GCST004860| X_107370118_A_G|           2.0|            -7|     X_107107919_A_G|  0.73615|[Subsignificant p...|\n",
      "|GCST004860| X_107370118_A_G|           2.0|            -7|     X_107408907_G_C| 0.725248|[Subsignificant p...|\n",
      "|GCST004860| 15_69857219_C_T|           8.0|            -6|     15_69877594_T_C| 0.755547|[Subsignificant p...|\n",
      "|GCST004860|  1_15410646_G_A|           6.0|            -6|      1_15375938_C_T| 0.761827|[Subsignificant p...|\n",
      "|GCST004860| X_106585073_C_T|           8.0|            -6|     X_107161940_T_G|  0.87271|[Subsignificant p...|\n",
      "|GCST004860|10_119025482_G_T|           7.0|            -6|    10_119029218_G_A| 0.799814|[Subsignificant p...|\n",
      "|GCST004860|10_119025482_G_T|           7.0|            -6|    10_119068783_G_C| 0.770287|[Subsignificant p...|\n",
      "|GCST004860|10_119025482_G_T|           7.0|            -6|    10_119104427_C_A|-0.716737|[Subsignificant p...|\n",
      "|GCST004860| X_107289163_G_C|           2.0|            -8|     X_107155425_G_A| 0.912979|[Palindrome allel...|\n",
      "|GCST004860| X_107289163_G_C|           2.0|            -8|     X_107060786_G_A| 0.893824|[Palindrome allel...|\n",
      "|GCST004860| X_107289163_G_C|           2.0|            -8|     X_107329405_A_G| 0.882586|[Palindrome allel...|\n",
      "|GCST004860| X_107289163_G_C|           2.0|            -8|    X_107056899_G_GA| 0.801232|[Palindrome allel...|\n",
      "|GCST004860| 16_75188108_G_C|           5.0|            -6|     16_75186365_C_T| 0.942803|[Subsignificant p...|\n",
      "|GCST004860| 7_142652944_A_G|           6.0|            -7|     7_142666075_A_G|  0.94088|[Subsignificant p...|\n",
      "|GCST004860| 7_142652944_A_G|           6.0|            -7|     7_142666532_T_G| 0.939227|[Subsignificant p...|\n",
      "+----------+----------------+--------------+--------------+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "study = ''\n",
    "ld_set = spark.read.parquet(\"gs://genetics_etl_python_playground/XX.XX/output/python_etl/parquet/pics_credible_set_not_clumped/\")\n",
    "# ld_set.show()\n",
    "\n",
    "\n",
    "dataset = (\n",
    "    ld_set\n",
    "    .select('studyId', 'variantId', 'pValueMantissa', 'pValueExponent', 'tagVariantId', 'R_overall', 'qualityControl')\n",
    "    .distinct()\n",
    "    .filter(f.col('studyId') == 'GCST004860')\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de3a7304373a2ed386fe951c9137ef8d6c9c0656a76027db0e908c100510c9c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
