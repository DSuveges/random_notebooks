{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T23:08:08.376271Z",
     "start_time": "2021-04-12T23:08:00.167649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "\n",
    "global spark\n",
    "\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '20g')\n",
    "\n",
    "spark = (pyspark.sql.SparkSession\n",
    "    .builder\n",
    "    .appName(\"phenodigm_parser\")\n",
    "    .config(\"spark.executor.memory\", '10g')\n",
    "     .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "    .config(\"spark.driver.memory\", '10g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "#   \n",
    "\n",
    "\n",
    "print('Spark version: ', spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T23:45:04.452190Z",
     "start_time": "2021-04-12T23:45:04.234554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- rsid: string (nullable = true)\n",
      "\n",
      "+---+-----+-----+--------+\n",
      "| id|label|value|    rsid|\n",
      "+---+-----+-----+--------+\n",
      "|  1|  foo|    2| rs16245|\n",
      "|  2|  bar|   12|  rs2344|\n",
      "|  2|  bar|  212|rs232983|\n",
      "|  2|  bar|    1| rs20398|\n",
      "|  3| cica|   12|   rs239|\n",
      "|  4| mica|   12| rs12235|\n",
      "+---+-----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        (1, \"foo\", 2, \"rs16245\"), \n",
    "        (2, \"bar\", 12, \"rs2344\"),\n",
    "        (2, \"bar\", 212, \"rs232983\"), \n",
    "        (2, \"bar\", 1, \"rs20398\"),\n",
    "        (3, \"cica\", 12, \"rs239\"), \n",
    "        (4, \"mica\", 12, \"rs12235\")\n",
    "    ],\n",
    "    [\"id\", \"label\", \"value\", \"rsid\"]  # add your column names here\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T07:13:03.013533Z",
     "start_time": "2021-04-13T07:13:02.999792Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reduceByKey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9fe93b839822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m (\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1401\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reduceByKey'"
     ]
    }
   ],
   "source": [
    "unique_fields = ['id','label']\n",
    "\n",
    "(\n",
    "    df\n",
    "    .reduceByKey(unique_fields)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T00:08:33.448183Z",
     "start_time": "2021-04-13T00:08:33.435352Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'parallelize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8e02fdedfa76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data_file = spark.parallelize([\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0;34m(\u001b[0m\u001b[0;34mu'u1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu's1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mu'u1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu's2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    (u'u2', u's3', 5), (u'u2', u's2', 10)])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m max_by_group = (data_file\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'parallelize'"
     ]
    }
   ],
   "source": [
    "data_file = spark.parallelize([\n",
    "   (u'u1', u's1', 20), \n",
    "   (u'u1', u's2', 5),\n",
    "   (u'u2', u's3', 5), \n",
    "   (u'u2', u's2', 10)])\n",
    "\n",
    "max_by_group = (data_file\n",
    "  .map(lambda x: (x[0], x))  # Convert to PairwiseRD\n",
    "  # Take maximum of the passed arguments by the last element (key)\n",
    "  # equivalent to:\n",
    "  # lambda x, y: x if x[-1] > y[-1] else y\n",
    "  .reduceByKey(lambda x1, x2: max(x1, x2, key=lambda x: x[-1])) \n",
    "  .values()) # Drop keys\n",
    "\n",
    "max_by_group.collect()\n",
    "## [('u2', 's2', 10), ('u1', 's1', 20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T23:58:18.010409Z",
     "start_time": "2021-04-12T23:58:18.001763Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0f47e24f373a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'show'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
