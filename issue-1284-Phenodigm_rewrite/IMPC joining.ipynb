{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:04:00.418032Z",
     "start_time": "2021-02-14T22:03:38.886225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version:  3.0.0\n",
      "Number of human phenotypes: 12841\n",
      "Number of mouse phenotypes: 10536\n",
      "Number of human to mouse phenotype mappings: 477198\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1719.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 250.0 failed 1 times, most recent failure: Lost task 0.0 in stage 250.0 (TID 7909, c02zq14flvdn, executor driver): java.lang.RuntimeException: Duplicate map key MP:0002498 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:72)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:87)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Duplicate map key MP:0002498 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:72)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:87)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3dd09df8d013>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0mmouse_model_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1719.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 250.0 failed 1 times, most recent failure: Lost task 0.0 in stage 250.0 (TID 7909, c02zq14flvdn, executor driver): java.lang.RuntimeException: Duplicate map key MP:0002498 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:72)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:87)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3625)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2695)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2902)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Duplicate map key MP:0002498 was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:72)\n\tat org.apache.spark.sql.catalyst.util.ArrayBasedMapBuilder.put(ArrayBasedMapBuilder.scala:87)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:86)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:33)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage9.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "\n",
    "def pheno_parser(x):\n",
    "    match = re.match('(\\D+:\\d+) (.+)', x)\n",
    "    try:\n",
    "        return {\n",
    "            'mp_id': match[1] if match[1] else None,\n",
    "            'mp_term':  match[2] if match[2] else None\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'mp_id': None,\n",
    "            'mp_term':  None\n",
    "        }\n",
    "\n",
    "parse_phenotypes = udf(pheno_parser, MapType(StringType(), StringType()))\n",
    "\n",
    "\n",
    "global spark\n",
    "\n",
    "# SparkContext.setSystemProperty('spark.executor.memory', '20g')\n",
    "\n",
    "spark = (pyspark.sql.SparkSession\n",
    "  .builder\n",
    "  .appName(\"phenodigm_parser\")\n",
    " .config(\"spark.executor.memory\", '10g') \\\n",
    " .config(\"spark.driver.memory\", '10g') \\\n",
    "  .getOrCreate()\n",
    ")\n",
    "\n",
    "#   .config(\"spark.driver.bindAddress\", \"localhost\")\n",
    "\n",
    "\n",
    "print('Spark version: ', spark.version)\n",
    "\n",
    "\n",
    "# solr data folder :\n",
    "solr_data_folder = '/Users/dsuveges/project_data/phenodigm_solr_dump'\n",
    "solr_data_folder = '/home/dsuveges/project/test_data'\n",
    "\n",
    "\"\"\"\n",
    "The steps below \n",
    " * read the gene set\n",
    " * filter for human genes (rows with hgnc_id)\n",
    " * join with gene-gene linking table\n",
    "\n",
    "Resulting table has \n",
    " * human HGNC gene IDs\n",
    " * human HGNC gene symbol\n",
    " * mouse MGI gene id\n",
    "\"\"\"\n",
    "\n",
    "human_genes = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.gene')\n",
    "    .select('hgnc_gene_id', 'hgnc_gene_symbol')\n",
    "    .filter(col('hgnc_gene_id').isNotNull())\n",
    ")\n",
    "\n",
    "genes_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.gene_gene')\n",
    "    .select('hgnc_gene_id', 'gene_id')\n",
    "    .join(human_genes, on='hgnc_gene_id', how='inner')\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Ontology table contains the mapping between human an mouse\n",
    "phenotypes. Only those mouse phenotypes included that have human correspondent\n",
    "\n",
    "The mouse phenotype term is not included - that value comes from the models table\n",
    "\"\"\"\n",
    "ontolgy_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.ontology_ontology')\n",
    "    .select('hp_id','hp_term','mp_id')\n",
    ")\n",
    "\n",
    "print(f\"Number of human phenotypes: {ontolgy_table.select('hp_id').distinct().count()}\")\n",
    "print(f\"Number of mouse phenotypes: {ontolgy_table.select('mp_id').distinct().count()}\")\n",
    "print(f\"Number of human to mouse phenotype mappings: {ontolgy_table.count()}\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The step below\n",
    " * reads mouse model table.\n",
    " * joins with genes table\n",
    "\n",
    "Resulting table has\n",
    " * human HGNC gene IDs\n",
    " * human HGNC gene symbol\n",
    " * mouse MGI gene id\n",
    " * mouse model identifier\n",
    " * mouse model phenotype list\n",
    "\"\"\"\n",
    "mouse_model_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.mouse_model')\n",
    "    .select('model_id','model_phenotypes', 'marker_id')\n",
    "    .withColumnRenamed('marker_id', 'gene_id')\n",
    "    .withColumn('model_phenotype', explode(col('model_phenotypes')))\n",
    "    .withColumn('parsed_phenotype', parse_phenotypes(col('model_phenotype')))\n",
    "    .drop('model_phenotypes')\n",
    "    .select('model_id', 'gene_id', 'parsed_phenotype.mp_id', 'parsed_phenotype.mp_term')\n",
    "    .join(ontolgy_table, on='mp_id', how='left')\n",
    "    .join(genes_table, on='gene_id', how='inner')\n",
    "    .groupby('model_id','gene_id', 'hgnc_gene_id', 'hgnc_gene_symbol')\n",
    "    .agg(\n",
    "        collect_set(struct(           \n",
    "                col(\"mp_id\").alias('id'), \n",
    "                col('mp_term').alias('label')\n",
    "            )).alias('diseaseModelAssociatedModelPhenotypes'),\n",
    "        collect_set(struct(           \n",
    "                col(\"hp_id\").alias('id'), \n",
    "                col('hp_term').alias('label')\n",
    "            )).alias('diseaseModelAssociatedHumanPhenotypes')        \n",
    "    )\n",
    ")\n",
    "mouse_model_table.show()\n",
    "\n",
    "# \"\"\"\n",
    "# The step below:\n",
    "#  * Opening disease model summary - table with disease terms for every model\n",
    "#  * Joined with mouse model table.\n",
    "\n",
    "# Resulting table adds the following columns:\n",
    "#  * disease_id\n",
    "#  * disease_model_avg_raw\n",
    "#  * disease_model_max_norm\n",
    "#  * disease_model_max_raw\n",
    "#  * disease_term\n",
    "#  * marker_id\n",
    "#  * marker_num_models\n",
    "#  * model_description\n",
    "#  * model_genetic_background\n",
    "#  * model_id\n",
    "# \"\"\"\n",
    "\n",
    "# # For now,I'm not sure if it make sense to do the aggregation.\n",
    "# # Aggregation was required because the data was stored in dictionaries\n",
    "disease_model_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.disease_model_summary/')\n",
    "    .drop(*['association_curated', 'marker_locus', 'marker_symbol','model_source','type', 'disease_model_avg_norm'])\n",
    "    .join(mouse_model_table, on='model_id', how='inner')\n",
    ")\n",
    "\n",
    "\n",
    "# (\n",
    "#     disease_model_table\n",
    "#     .select('association_curated', 'disease_id', 'disease_term', 'marker_id', 'marker_num_models','model_description', 'model_genetic_background', 'model_id')\n",
    "#     .where(col('model_id') == 'MGI:2678495')\n",
    "#     .show()\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# f = (\n",
    "#     disease_model_table\n",
    "#     .join(mouse_model_table, on='model_id', how='inner')\n",
    "# )\n",
    "\n",
    "# print(models_diseases_joined.count())\n",
    "# print(models_diseases_joined.show())\n",
    "# disease_model_table.show()\n",
    "\n",
    "\n",
    "# mouse_model_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T10:21:24.337327Z",
     "start_time": "2021-02-14T10:21:22.324873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29310\n",
      "29310\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ontology_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.ontology/')\n",
    ")\n",
    "\n",
    "print(ontology_table.count())\n",
    "print(ontology_table.select('phenotype_id').distinct().count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T22:44:32.603538Z",
     "start_time": "2021-02-10T22:44:31.961337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4797"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reading disease\n",
    "\"\"\"\n",
    "disease_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.disease/')\n",
    "    .select('disease_id','disease_phenotypes','disease_term')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-13T21:15:05.848094Z",
     "start_time": "2021-02-13T21:15:05.829797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disease_gene_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.disease_gene_summary/')\n",
    ").show()\n",
    "\n",
    "\n",
    "disease_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.disease')\n",
    ")\n",
    "\n",
    "disease_table.show()\n",
    "\n",
    "ontolgy_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.ontology')\n",
    "    .select('phenotype_id','phenotype_term')\n",
    ")\n",
    "\n",
    "ontolgy_table.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ontolgy_ontology_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    disease_gene_table\n",
    "    .filter(col('disease_id') == 'OMIM:101600')\n",
    "    .select('hgnc_gene_symbol')\n",
    "    .distinct()\n",
    "    .show()\n",
    ")\n",
    "\n",
    "(\n",
    "    disease_model_table\n",
    "    .filter(col('disease_id') == 'OMIM:101600')\n",
    "    .select('hgnc_gene_symbol')\n",
    "    .distinct()\n",
    "    .sort('hgnc_gene_symbol')\n",
    "    .show()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:03:24.441066Z",
     "start_time": "2021-02-14T00:03:11.454297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of human phenotypes: 12841\n",
      "Number of mouse phenotypes: 10536\n",
      "Number of human to mouse phenotype mappings: 477198\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ontology table contains the mapping between human an mouse\n",
    "phenotypes.\n",
    "\"\"\"\n",
    "ontolgy_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.ontology_ontology')\n",
    "    .select('hp_id','hp_term','mp_id','mp_term')\n",
    ")\n",
    "\n",
    "print(f\"Number of human phenotypes: {ontolgy_table.select('hp_id').distinct().count()}\")\n",
    "print(f\"Number of mouse phenotypes: {ontolgy_table.select('mp_id').distinct().count()}\")\n",
    "print(f\"Number of human to mouse phenotype mappings: {ontolgy_table.count()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T10:28:53.072969Z",
     "start_time": "2021-02-14T10:28:52.984336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+\n",
      "|     hp_id|             hp_term|     mp_id|             mp_term|\n",
      "+----------+--------------------+----------+--------------------+\n",
      "|HP:0006704|Abnormal coronary...|MP:0002845|abnormal aortic w...|\n",
      "|HP:0006704|Abnormal coronary...|MP:0002844|  aortic hypertrophy|\n",
      "|HP:0006704|Abnormal coronary...|MP:0002725|abnormal vein mor...|\n",
      "|HP:0006704|Abnormal coronary...|MP:0005339|increased suscept...|\n",
      "|HP:0006704|Abnormal coronary...|MP:0005338|atherosclerotic l...|\n",
      "|HP:0006704|Abnormal coronary...|MP:0012255|abnormal intersom...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0011680|tricuspid valve h...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010484|bicuspid aortic v...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010596|unicuspid aortic ...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010628|patent tricuspid ...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010629|thick tricuspid v...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010627|enlarged tricuspi...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010626|thick tricuspid v...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010625|absent tricuspid ...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010624|unicuspid tricusp...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010623|bicuspid tricuspi...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010622|abnormal tricuspi...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010621|abnormal tricuspi...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010611|patent pulmonary ...|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010606|abnormal pulmonar...|\n",
      "+----------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ontolgy_table.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T00:53:41.270294Z",
     "start_time": "2021-02-14T00:53:41.265341Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def udf_test(n):\n",
    "    return (n / 2, n % 2) if n and n != 0.0 else (float('nan'), float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T01:03:48.111242Z",
     "start_time": "2021-02-14T01:03:48.100854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MP:0003091 abnormal cell migration'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phenotypes = ['MP:0002825 abnormal notochord morphology',\n",
    " 'MP:0005221 abnormal rostral-caudal axis patterning',\n",
    " 'MP:0003091 abnormal cell migration',\n",
    " 'MP:0001680 abnormal mesoderm development',\n",
    " 'MP:0001675 abnormal ectoderm development',\n",
    " 'MP:0011260 abnormal head mesenchyme morphology',\n",
    " 'MP:0012104 small amniotic cavity',\n",
    " 'MP:0004787 abnormal dorsal aorta morphology',\n",
    " 'MP:0001698 decreased embryo size',\n",
    " 'MP:0000272 abnormal aorta morphology',\n",
    " 'MP:0004180 failure of initiation of embryo turning',\n",
    " 'MP:0010656 thick myocardium',\n",
    " 'MP:0001614 abnormal blood vessel morphology',\n",
    " 'MP:0002128 abnormal blood circulation',\n",
    " 'MP:0002086 abnormal extraembryonic tissue morphology',\n",
    " 'MP:0003974 abnormal endocardium morphology',\n",
    " 'MP:0004261 abnormal embryonic neuroepithelium morphology',\n",
    " 'MP:0005029 abnormal amnion morphology',\n",
    " 'MP:0000267 abnormal heart development',\n",
    " 'MP:0010547 abnormal mesocardium morphology',\n",
    " 'MP:0009657 failure of chorioallantoic fusion',\n",
    " 'MP:0012131 small visceral yolk sac',\n",
    " 'MP:0001726 abnormal allantois morphology',\n",
    " 'MP:0000474 abnormal foregut morphology',\n",
    " 'MP:0002085 abnormal embryonic tissue morphology',\n",
    " 'MP:0001914 hemorrhage',\n",
    " 'MP:0002084 abnormal developmental patterning',\n",
    " 'MP:0010664 abnormal vitelline artery morphology',\n",
    " 'MP:0011257 abnormal head fold morphology',\n",
    " 'MP:0011098 embryonic lethality during organogenesis, complete penetrance',\n",
    " 'MP:0001688 abnormal somite development',\n",
    " 'MP:0011201 abnormal visceral yolk sac cavity morphology',\n",
    " 'MP:0000358 abnormal cell morphology']\n",
    "\n",
    "import re \n",
    "\n",
    "x = phenotypes[2]\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T01:07:44.948508Z",
     "start_time": "2021-02-14T01:07:44.944662Z"
    }
   },
   "outputs": [],
   "source": [
    "b = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T21:32:32.339702Z",
     "start_time": "2021-02-14T21:32:32.334755Z"
    }
   },
   "outputs": [],
   "source": [
    "parse_phenotypes = udf(\n",
    "    lambda x: {'id': re.match('(\\D+:\\d+) (.+)', x)[1], 'name': re.match('(MP:\\d+) (.+)', x)[2]},\n",
    ")\n",
    "\n",
    "def pheno_parser(x):\n",
    "    match = re.match('(\\D+:\\d+) (.+)', x)\n",
    "    try:\n",
    "        return {\n",
    "            'mp_id': match[1] if match[1] else None,\n",
    "            'mp_term':  match[2] if match[2] else None,\n",
    "        }\n",
    "    except:\n",
    "        print(x)\n",
    "\n",
    "parse_phenotypes = udf(pheno_parser, StructType())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T10:10:13.074822Z",
     "start_time": "2021-02-14T10:10:13.051437Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dsuveges/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1200, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/dsuveges/opt/anaconda3/envs/jupyter/lib/python3.6/socket.py\", line 586, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dsuveges/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/dsuveges/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o23.read",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-780670371e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mouse_model_table = (\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{solr_data_folder}/type.mouse_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'model_phenotypes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'marker_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'marker_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gene_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataFrameReader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \"\"\"\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrameReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jupyter/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o23.read"
     ]
    }
   ],
   "source": [
    "mouse_model_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.mouse_model')\n",
    "    .limit(1000)\n",
    "    .select('model_id','model_phenotypes', 'marker_id')\n",
    "    .withColumnRenamed('marker_id', 'gene_id')\n",
    "    .join(genes_table, on='gene_id', how='inner')\n",
    "    .withColumn('model_phenotype', explode(col('model_phenotypes')))\n",
    "    .withColumn('mp_id', split(col('model_phenotype'),' ').getItem(0))\n",
    "    .join(ontolgy_ontology_table, on='mp_id', how='left')\n",
    ")\n",
    "pdf = mouse_model_table.toPandas()\n",
    "pdf.head()\n",
    "\n",
    "mouse_model_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.mouse_model')\n",
    "    .select('model_id','model_phenotypes', 'marker_id')\n",
    "    .withColumnRenamed('marker_id', 'gene_id')\n",
    "    .withColumn('model_phenotype', explode(col('model_phenotypes')))\n",
    "    .withColumn('parsed_phenotype', parse_phenotypes(col('model_phenotype')))\n",
    "    .select('model_id', 'gene_id', 'parsed_phenotype.mp_id', 'parsed_phenotype.mp_term')\n",
    ")\n",
    "mouse_model_table.show()\n",
    "\n",
    "pdf = mouse_model_table.toPandas()\n",
    "pdf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T01:20:24.517302Z",
     "start_time": "2021-02-14T01:20:24.514130Z"
    }
   },
   "outputs": [],
   "source": [
    "match = re.match('(\\D+:\\d+) (.+)', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T09:49:56.536527Z",
     "start_time": "2021-02-14T09:49:54.886359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|phenotype_id|      phenotype_term|\n",
      "+------------+--------------------+\n",
      "|  HP:0000001|                 All|\n",
      "|  HP:0000002|Abnormality of bo...|\n",
      "|  HP:0000003|Multicystic kidne...|\n",
      "|  HP:0000005| Mode of inheritance|\n",
      "|  HP:0000006|Autosomal dominan...|\n",
      "|  HP:0000007|Autosomal recessi...|\n",
      "|  HP:0000008|Abnormal morpholo...|\n",
      "|  HP:0000009|Functional abnorm...|\n",
      "|  HP:0000010|Recurrent urinary...|\n",
      "|  HP:0000011|  Neurogenic bladder|\n",
      "|  HP:0000012|     Urinary urgency|\n",
      "|  HP:0000013|Hypoplasia of the...|\n",
      "|  HP:0000014|Abnormality of th...|\n",
      "|  HP:0000015|Bladder diverticulum|\n",
      "|  HP:0000016|   Urinary retention|\n",
      "|  HP:0000017|            Nocturia|\n",
      "|  HP:0000019|   Urinary hesitancy|\n",
      "|  HP:0000020|Urinary incontinence|\n",
      "|  HP:0000021|          Megacystis|\n",
      "|  HP:0000022|Abnormality of ma...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ontolgy_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.ontology')\n",
    "    .select('phenotype_id','phenotype_term')\n",
    ")\n",
    "\n",
    "ontolgy_table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T09:50:51.970712Z",
     "start_time": "2021-02-14T09:50:43.777989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+--------------------+-----------------+\n",
      "|     hp_id|             hp_term|     mp_id|             mp_term|             type|\n",
      "+----------+--------------------+----------+--------------------+-----------------+\n",
      "|HP:0006704|Abnormal coronary...|MP:0002845|abnormal aortic w...|ontology_ontology|\n",
      "|HP:0006704|Abnormal coronary...|MP:0002844|  aortic hypertrophy|ontology_ontology|\n",
      "|HP:0006704|Abnormal coronary...|MP:0002725|abnormal vein mor...|ontology_ontology|\n",
      "|HP:0006704|Abnormal coronary...|MP:0005339|increased suscept...|ontology_ontology|\n",
      "|HP:0006704|Abnormal coronary...|MP:0005338|atherosclerotic l...|ontology_ontology|\n",
      "|HP:0006704|Abnormal coronary...|MP:0012255|abnormal intersom...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0011680|tricuspid valve h...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010484|bicuspid aortic v...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010596|unicuspid aortic ...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010628|patent tricuspid ...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010629|thick tricuspid v...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010627|enlarged tricuspi...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010626|thick tricuspid v...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010625|absent tricuspid ...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010624|unicuspid tricusp...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010623|bicuspid tricuspi...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010622|abnormal tricuspi...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010621|abnormal tricuspi...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010611|patent pulmonary ...|ontology_ontology|\n",
      "|HP:0006705|Abnormal atrioven...|MP:0010606|abnormal pulmonar...|ontology_ontology|\n",
      "+----------+--------------------+----------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ontolgy_ontology_table = (\n",
    "    spark.read.json(f'{solr_data_folder}/type.ontology_ontology')\n",
    ")\n",
    "\n",
    "ontolgy_ontology_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T10:06:32.145823Z",
     "start_time": "2021-02-14T10:06:31.927352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       split|\n",
      "+------------+\n",
      "|[A, B, C, D]|\n",
      "|      [E, F]|\n",
      "+------------+\n",
      "\n",
      "+------------+--------+\n",
      "|       split|lastItem|\n",
      "+------------+--------+\n",
      "|[A, B, C, D]|       A|\n",
      "|      [E, F]|       E|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([[['A', 'B', 'C', 'D']], [['E', 'F']]], ['split'])\n",
    "df.show()\n",
    "# +------------+\n",
    "# |       split|\n",
    "# +------------+\n",
    "# |[A, B, C, D]|\n",
    "# |      [E, F]|\n",
    "# +------------+\n",
    "\n",
    "# import pyspark.sql.functions as F\n",
    "df.withColumn('lastItem', col('split').getItem(0)).show()\n",
    "# +------------+--------+\n",
    "# |       split|lastItem|\n",
    "# +------------+--------+\n",
    "# |[A, B, C, D]|       D|\n",
    "# |      [E, F]|       F|\n",
    "# +------------+--------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:11:39.327352Z",
     "start_time": "2021-02-14T22:11:38.601067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| id|key|value|\n",
      "+---+---+-----+\n",
      "|  1|  a|  123|\n",
      "|  1|  b|  234|\n",
      "|  1|  c|  345|\n",
      "|  2|  a|   12|\n",
      "|  2|  x|   23|\n",
      "|  2|  y|  123|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "toy_data = spark.createDataFrame([\n",
    "    Row(id=1, key='a', value=\"123\"),\n",
    "    Row(id=1, key='b', value=\"234\"),\n",
    "    Row(id=1, key='c', value=\"345\"),\n",
    "    Row(id=2, key='a', value=\"12\"),\n",
    "    Row(id=2, key='x', value=\"23\"),\n",
    "    Row(id=2, key='y', value=\"123\")])\n",
    "\n",
    "toy_data.show()\n",
    "\n",
    "# +---+---+-----+\n",
    "# | id|key|value|\n",
    "# +---+---+-----+\n",
    "# |  1|  a|  123|\n",
    "# |  1|  b|  234|\n",
    "# |  1|  c|  345|\n",
    "# |  2|  a|   12|\n",
    "# |  2|  x|   23|\n",
    "# |  2|  y|  123|\n",
    "# +---+---+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:46:17.018722Z",
     "start_time": "2021-02-14T22:46:16.992169Z"
    }
   },
   "outputs": [],
   "source": [
    "upd = (\n",
    "    toy_data\n",
    "    .groupby('id')\n",
    "    .agg(\n",
    "        map_from_entries(\n",
    "            collect_set( \n",
    "                struct(           \n",
    "                    col(\"key\").alias('id'), \n",
    "                    col('value').alias('value')\n",
    "                )\n",
    "            )\n",
    "        ).alias('key_value')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:49:52.241662Z",
     "start_time": "2021-02-14T22:49:51.949108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           key_value|\n",
      "+---+--------------------+\n",
      "|  1|[c -> 345, a -> 1...|\n",
      "|  2|[y -> 123, x -> 2...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(json.dumps(json.loads(upd.schema.json()),indent=2))\n",
    "upd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:56:36.217047Z",
     "start_time": "2021-02-14T22:56:36.193480Z"
    }
   },
   "outputs": [],
   "source": [
    "upd = (\n",
    "    toy_data\n",
    "    .groupby('id')\n",
    "    .agg(\n",
    "        collect_set(                \n",
    "            struct(           \n",
    "                col(\"key\").alias('id'), \n",
    "                col('value').alias('value')\n",
    "            )\n",
    "        ).alias('col_header')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:56:42.874853Z",
     "start_time": "2021-02-14T22:56:42.211116Z"
    }
   },
   "outputs": [],
   "source": [
    "upd.write.json('cicaful1.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T22:56:53.841637Z",
     "start_time": "2021-02-14T22:56:53.822717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":1,\"col_header\":[{\"id\":\"c\",\"value\":\"345\"},{\"id\":\"a\",\"value\":\"123\"},{\"id\":\"b\",\"value\":\"234\"}]}\n",
      "{\"id\":2,\"col_header\":[{\"id\":\"y\",\"value\":\"123\"},{\"id\":\"x\",\"value\":\"23\"},{\"id\":\"a\",\"value\":\"12\"}]}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat cicaful1.json/*json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
