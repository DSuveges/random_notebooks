{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rewite PhenoDigm parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:23:45.823877Z",
     "start_time": "2021-02-08T13:23:44.811360Z"
    },
    "code_folding": [
     19,
     80
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from retry import retry\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# class IPMC_solr_parser(object):\n",
    "#     '''\n",
    "#     This class retrieves data from the IPMC solr API.\n",
    "    \n",
    "#     * Returns array of documents.\n",
    "#     * If target folder is specified, json files are saved into that folder.\n",
    "#     '''\n",
    "    \n",
    "#     ipmc_solr_host = 'http://www.ebi.ac.uk/mi/impc/solr/phenodigm/select'\n",
    "\n",
    "    \n",
    "#     def __init__(self, rows=20000, limit=None, target_folder=None):\n",
    "\n",
    "#         start = 0\n",
    "#         rows = 20000\n",
    "#         total = 0\n",
    "#         numFound = 1\n",
    "#         chunk = 0\n",
    "        \n",
    "#         timing = []\n",
    "\n",
    "#         logging.info(f'Retrieving data from IPMC solr: {self.ipmc_solr_host}')\n",
    "#         logging.info(f'Retrieving {rows} documents at a time.')\n",
    "#         self.data = []\n",
    "            \n",
    "#         # Retrieving all data from IPMC:\n",
    "#         while True:\n",
    "#             start_time = datetime.now()\n",
    "            \n",
    "#             # Retrieve data:\n",
    "#             rsp = self.query_solr(start, rows, mode)\n",
    "            \n",
    "#             # Increment chunk:\n",
    "       \n",
    "#             # We can manually limit the number of documents:\n",
    "#             if not limit:\n",
    "#                 limit = rsp['response']['numFound']\n",
    "            \n",
    "#             # If we don't find any items, we break:\n",
    "#             if rsp['response']['numFound'] == 0:\n",
    "#                 break\n",
    "                \n",
    "#             # Store data of write to file:\n",
    "#             if target_folder:\n",
    "#                 with open(f'{target_folder}/IMPC_solr_dump.{chunk:03}.json', 'w') as f:\n",
    "#                     for doc in rsp['response']['docs']:\n",
    "#                         json.dump(doc, f)\n",
    "#                         f.write('\\n')\n",
    "#             else:\n",
    "#                 self.data += rsp['response']['docs']\n",
    "\n",
    "#             # Incrementing starting position\n",
    "#             start += rows\n",
    "#             chunk += 1\n",
    "#             total += len(rsp['response']['docs'])\n",
    "            \n",
    "#             # If the length of the dataframe reaches the limit, we exit:\n",
    "#             if limit <= total:\n",
    "#                 break\n",
    "            \n",
    "#             # Log progress\n",
    "#             second_last = (datetime.now() - start_time).total_seconds()\n",
    "#             logging.debug(f'Chunk {chunk} done. Number of retrieved documents: {total}, last step took: {second_last} seconds.')\n",
    "            \n",
    "#             timing.append({'docs': total, 'time': second_last})\n",
    "         \n",
    "#         self.timing = pd.DataFrame(timing)\n",
    "#         logging.info(f'Retrieval finished. Number of documents: {len(self.data)}')\n",
    "            \n",
    "\n",
    "#     # Use @retry decorator to ensure that errors like the query failing because server was overloaded, are handled correctly and the request is retried\n",
    "#     @retry(tries=3, delay=5, backoff=1.2, jitter=(1, 3))\n",
    "#     def query_solr(self, start, rows, mode):\n",
    "\n",
    "#         # Building request:\n",
    "#         params = {'q': '*:*','start': start,'rows': rows}\n",
    "  \n",
    "#         params = dict(q=\"*\", start=start, rows=rows)\n",
    "#         if mode == 'update_cache':\n",
    "#             params['fq'] = 'type:gene'\n",
    "            \n",
    "#         # Query\n",
    "#         r = requests.get(self.ipmc_solr_host, params=params, timeout=30)\n",
    "\n",
    "#         # Check for erroneous HTTP response statuses\n",
    "#         r.raise_for_status()\n",
    "#         rsp = r.json()\n",
    "#         return rsp\n",
    "\n",
    "#     def get_data(self):\n",
    "#         return self.data.copy()\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:24:09.131119Z",
     "start_time": "2021-02-08T13:24:09.117656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<StreamHandler stdout (NOTSET)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "logging.shutdown()\n",
    "reload(logging)\n",
    "\n",
    "# Initialize logger:\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s %(levelname)s %(module)s - %(funcName)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    ")\n",
    "\n",
    "logging.StreamHandler(sys.stdout)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the data into compressed csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T22:56:42.168263Z",
     "start_time": "2021-01-26T22:53:29.235091Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv('impc_full_dataset.tsv', sep='\\t', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T06:54:03.412610Z",
     "start_time": "2021-01-27T06:53:53.929477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['gene_gene', 'gene', 'disease_search', 'ontology',\n",
       "       'disease_gene_summary', 'ontology_ontology', 'disease',\n",
       "       'disease_model_summary', 'mouse_model'], dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T13:25:29.310083Z",
     "start_time": "2021-02-08T13:25:29.294583Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "class IPMC_solr_parser(object):\n",
    "    '''\n",
    "    This class retrieves data from the IPMC solr API.\n",
    "    \n",
    "    * Returns array of documents.\n",
    "    * If target folder is specified, json files are saved into that folder.\n",
    "    '''\n",
    "    \n",
    "    ipmc_solr_host = 'http://www.ebi.ac.uk/mi/impc/solr/phenodigm/select'\n",
    "\n",
    "    \n",
    "    def __init__(self, target_folder, rows=20000, limit=None):\n",
    "        \"\"\"\n",
    "        Storing basic values when initializing object\n",
    "        \n",
    "        Args:\n",
    "        rows (int): Number of solr documents returned in a single query\n",
    "        limit (int): Maximum number of returned document. If None, all documents are returned\n",
    "        target_folder (string): Folder into which the data is saved.\n",
    "        \n",
    "        Returns: \n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.rows = rows\n",
    "        self.limit = limit\n",
    "        self.target_folder = target_folder\n",
    "        \n",
    "        \n",
    "    def fetch_data(self, data_type=None):\n",
    "        \"\"\"\n",
    "        Fetching data to the specified location. If data type is not specified, \n",
    "        all types are retrieved and the files are saved directly to the root folder\n",
    "        \n",
    "        Args:\n",
    "        data_type (string): data type to return match .type == 'data_type'\n",
    "        \n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Based on the data type, we update the output folder:\n",
    "        data_folder = f'{self.target_folder}/type.{data_type}' if data_type else self.target_folder\n",
    "            \n",
    "        # Create folder:\n",
    "        os.makedirs(data_folder, exist_ok=True) \n",
    "        \n",
    "        # Initialize counter:\n",
    "        start = 0\n",
    "        total = 0\n",
    "        numFound = 1\n",
    "        chunk = 0\n",
    "        limit = None\n",
    "\n",
    "        logging.info(f'Retrieving data from IPMC solr: {self.ipmc_solr_host}')\n",
    "        logging.info(f'Retrieving {self.rows} documents at a time.')\n",
    "        logging.info(f'Specified data type: {data_type}')\n",
    "            \n",
    "        # Retrieving data from IMPC:\n",
    "        while True:\n",
    "            \n",
    "            # Retrieve data:\n",
    "            solr_data = self.query_solr(start, data_type)\n",
    "       \n",
    "            # If limit is not set, return all data:\n",
    "            if not limit:\n",
    "                limit = solr_data['response']['numFound'] if not self.limit else self.limit\n",
    "            \n",
    "            # If we don't find any items, we break:\n",
    "            if solr_data['response']['numFound'] == 0:\n",
    "                break\n",
    "                \n",
    "            # Write data to file:\n",
    "            with gzip.open(f'{data_folder}/IMPC_solr_dump.{chunk:03}.json.gz', 'tw') as f:\n",
    "                for doc in solr_data['response']['docs']:\n",
    "                    json.dump(doc, f)\n",
    "                    f.write('\\n')\n",
    "\n",
    "            # Incrementing counters:\n",
    "            start += self.rows\n",
    "            chunk += 1\n",
    "            total += len(solr_data['response']['docs'])\n",
    "            \n",
    "            # If the length of the dataframe reaches the limit, we exit:\n",
    "            if (limit <= total) or (total == solr_data['response']['numFound']):\n",
    "                break\n",
    "            \n",
    "            # Log progress\n",
    "            logging.debug(f'Chunk {chunk} done. Number of retrieved documents: {total}.')\n",
    "\n",
    "            \n",
    "        logging.info(f'Retrieval finished. Number of documents: {total}')\n",
    "            \n",
    "\n",
    "    # Use @retry decorator to ensure that errors like the query failing because server was overloaded, are handled correctly and the request is retried\n",
    "    @retry(tries=3, delay=5, backoff=1.2, jitter=(1, 3))\n",
    "    def query_solr(self, start, data_type=None):\n",
    "\n",
    "        # Building request:\n",
    "        params = {'q': '*:*','start': start,'rows': self.rows}\n",
    "\n",
    "        if data_type:\n",
    "            params['fq'] = f'type:{data_type}'\n",
    "            \n",
    "        # Query\n",
    "        r = requests.get(self.ipmc_solr_host, params=params, timeout=30)\n",
    "\n",
    "        # Check for erroneous HTTP response statuses\n",
    "        r.raise_for_status()\n",
    "        rsp = r.json()\n",
    "        return rsp\n",
    "\n",
    "\n",
    "# target_folder = '/Users/dsuveges/project/random_notebooks/issue-1284-Phenodigm_rewrite/cicaful'\n",
    "# impc_solr_retriever = IPMC_solr_parser(rows=20000, limit=50000, target_folder=target_folder)\n",
    "# impc_solr_retriever.fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:04:42.118545Z",
     "start_time": "2021-02-08T13:25:49.455107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching: gene\n",
      "fetching: gene_gene\n",
      "fetching: mouse_model\n",
      "fetching: disease_model_summary\n",
      "fetching: disease_gene_summary\n",
      "fetching: disease\n",
      "fetching: ontology_ontology\n",
      "fetching: ontology\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "# Retrieving samples from every data types:\n",
    "# limit = 100000\n",
    "target_folder = '/Users/dsuveges/project/random_notebooks/issue-1284-Phenodigm_rewrite/cicaful'\n",
    "\n",
    "data_types = ['gene','gene_gene','mouse_model','disease_model_summary',\n",
    "              'disease_gene_summary','disease','ontology_ontology','ontology']\n",
    "\n",
    "\n",
    "# Initialize impc solr object:\n",
    "impc_solr_retriever = IPMC_solr_parser(rows=20000, target_folder=target_folder)\n",
    "\n",
    "for data_type in data_types:\n",
    "    print(f'fetching: {data_type}')\n",
    "    impc_solr_retriever.fetch_data(data_type=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-06T00:18:06.942763Z",
     "start_time": "2021-02-06T00:17:47.998038Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize impc solr object:\n",
    "impc_solr_retriever = IPMC_solr_parser(rows=20000, limit=1000000, target_folder=target_folder)\n",
    "impc_solr_retriever.fetch_data(data_type='mouse_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T14:37:49.373575Z",
     "start_time": "2021-02-08T14:37:49.336572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dsuveges/project/random_notebooks/issue-1284-Phenodigm_rewrite\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
