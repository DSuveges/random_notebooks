{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping the new json schema\n",
    "\n",
    "\n",
    "As part of the consolidation of the evidence objects in the backend, we are re-modeling the [json schema](https://github.com/opentargets/json_schema) to reflect the new simplified/flattened design.\n",
    "\n",
    "**Link to ticket: [#1249](https://github.com/opentargets/platform/issues/1249)**\n",
    "\n",
    "Based on the meeting we had on 2020.11.11 the following conclusions were reached:\n",
    "\n",
    "* We need to maintain a json schema that guides our data providers and can be used as template to generate evidence strings.\n",
    "* The schema will reflect the concepts of the new platform design, so the units of the schema is going to be data source centric instead of data type.\n",
    "* Each of the valuable columns will be defined in a common section.\n",
    "* For each data source there will only be a list of required fields.\n",
    "* We haven't reached a consensus on how the unique association fields are defined, and at which point of the evidence generation. So for the first iteration of the json schema, the unique_association_fields will be omitted.\n",
    "\n",
    "The schema is written based on the most recent iteration of the [evidence schema review](https://docs.google.com/spreadsheets/d/11jdPCo_vxY3jaP54xKTsXBshR5HMrpUf5oXJNgtbKm8/edit#gid=1735847104) document.\n",
    "\n",
    "The technical approach:\n",
    "\n",
    "* To avoid manual work with the json document, I'm collating information in an excel file and will use that as a source for the definitions.\n",
    "* The same excel file will be used to get the source names from where we are expecting the given field.\n",
    "\n",
    "## The first run completed:\n",
    "\n",
    "- [X] processing the review document to get the rough list of fields\n",
    "- [x] get fields2datasource mapping\n",
    "- [x] generate json schema based on the meeting\n",
    "\n",
    "\n",
    "## The first run didn't cover:\n",
    "\n",
    "- [ ] some fields are missing eg. uniprot id\n",
    "- [ ] some fields shoudl not be here: `score` and `id`\n",
    "- [ ] the precise requirements of the fields are still sparse -> add more data to `field_description.xlsx`\n",
    "- [ ] no structure whatsoever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the list of data source for every field\n",
    "\n",
    "This information is extracted from the evidence schema review file. The end of the process is a comma separated list of data sources for every field. This column is used later to generate the mandatory list of fields for every data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T16:59:39.442220Z",
     "start_time": "2020-12-10T16:59:39.330332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allelicRequirements</td>\n",
       "      <td>genomics_england,clingen,eva,gene2phenotype,ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biologicalModelAllelicComposition</td>\n",
       "      <td>phenodigm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>biologicalModelGeneticBackground</td>\n",
       "      <td>phenodigm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clinicalPhase</td>\n",
       "      <td>chembl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clinicalSignificances</td>\n",
       "      <td>eva,eva_somatic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              column  \\\n",
       "0                allelicRequirements   \n",
       "1  biologicalModelAllelicComposition   \n",
       "2   biologicalModelGeneticBackground   \n",
       "3                      clinicalPhase   \n",
       "4              clinicalSignificances   \n",
       "\n",
       "                                             sources  \n",
       "0  genomics_england,clingen,eva,gene2phenotype,ev...  \n",
       "1                                          phenodigm  \n",
       "2                                          phenodigm  \n",
       "3                                             chembl  \n",
       "4                                    eva,eva_somatic  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Get the source names for every field:\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import OrderedDict, defaultdict\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "field_mapping_df = pd.read_csv('fields_sources.tsv', sep='\\t')\n",
    "field_mapping_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading field description\n",
    "\n",
    "1. Read excel file with the field descriptions: from [google sheets](https://docs.google.com/spreadsheets/d/1vHoyIsQDBNmUfq2IUdZDoz3G457V5cqW/edit#gid=613969206)\n",
    "2. Parse values. \n",
    "3. Start building json object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T20:24:30.712989Z",
     "start_time": "2020-12-14T20:24:30.693231Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_dataframe():\n",
    "    '''\n",
    "    This function fetches the field definitinos from google spreadsheets\n",
    "    '''\n",
    "    field_description = 'field_descriptions.tsv'\n",
    "\n",
    "    # The file from now on is stored on google sheets:\n",
    "    url = 'https://docs.google.com/spreadsheets/d/1vHoyIsQDBNmUfq2IUdZDoz3G457V5cqW/export?format=tsv&id=1vHoyIsQDBNmUfq2IUdZDoz3G457V5cqW&gid=613969206'\n",
    "    r = requests.get(url)\n",
    "\n",
    "    with open(field_description, 'w') as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "    fields_df = pd.read_csv(field_description, sep='\\t')\n",
    "    return fields_df\n",
    "\n",
    "\n",
    "def parse_data_sources(df):\n",
    "    '''\n",
    "    Parsing dataframe to get list of fields for each data source:\n",
    "    \n",
    "    input: features_df\n",
    "    output: oneof ordered dictionary\n",
    "    '''\n",
    "    \n",
    "    parsed_sources = defaultdict(list)\n",
    "    for i, row in df.iterrows():\n",
    "        for source in row['data_source'].split(','):                \n",
    "            parsed_sources[source].append(row['field_name'])\n",
    "\n",
    "    # Each data source then exploded into schemas:\n",
    "    source_schemas = []\n",
    "    sorted_items = sorted(parsed_sources.items())\n",
    "    for source, fields in sorted_items:\n",
    "        source_schema = OrderedDict()\n",
    "\n",
    "        # Adding property definitions:\n",
    "        source_schema['properties'] = OrderedDict({'datasourceId': {\"const\": source}})\n",
    "        \n",
    "        for field in fields:\n",
    "            if field == 'datasourceId':\n",
    "                continue\n",
    "                \n",
    "            source_schema['properties'][field] = {\"$ref\": f\"#/definitions/{field}\"}\n",
    "        \n",
    "        source_schema['required'] = ['datasourceId', 'targetFromSourceId', 'diseaseId']\n",
    "        source_schema[\"additionalProperties\"] = False\n",
    "\n",
    "        # Adding source schema:\n",
    "        source_schemas.append(source_schema)\n",
    "        \n",
    "    return(source_schemas)\n",
    "\n",
    "\n",
    "def add_definition(row, fields_df):\n",
    "    '''\n",
    "    This is the main function to add definitions to the schema\n",
    "    '''\n",
    "    \n",
    "    # If the feature is simple:\n",
    "    if row['type'] in ['string', 'integer', 'number']:\n",
    "        return add_simple_definition(row)\n",
    "    \n",
    "    # If the feature is complex:\n",
    "    elif row['type'] == 'array':\n",
    "        print(row[\"field_name\"])\n",
    "        return add_array(row, fields_df)\n",
    "        print(f'complex objectfound: {row[\"field_name\"]}')\n",
    "    \n",
    "    \n",
    "def add_array(row, df):\n",
    "    '''\n",
    "    If the definition is an array, things have to be treated separately\n",
    "    '''\n",
    "    field_annotation = OrderedDict({'type':'array'})\n",
    "    field_name = row['field_name']\n",
    "    \n",
    "    # If there's a description:\n",
    "    if isinstance(row['description'], str):\n",
    "        field_annotation['description'] = row['description']\n",
    "\n",
    "    # Items are objects: \n",
    "    if len(df.loc[df.location == field_name]) > 0:\n",
    "        field_annotation['items'] = OrderedDict({\n",
    "            'type': \"object\",\n",
    "            'properties': OrderedDict()\n",
    "        })\n",
    "        \n",
    "        sub_df = df.loc[df.location == field_name]\n",
    "        for index, sub_row in sub_df.iterrows():\n",
    "             field_annotation['items']['properties'][sub_row['field_name']] = add_definition(sub_row, sub_df)\n",
    "\n",
    "        \n",
    "    # Items are string:\n",
    "    else:\n",
    "        row['type'] = 'string'\n",
    "        row['description'] = None\n",
    "\n",
    "        field_annotation['items'] = add_simple_definition(row)\n",
    "        \n",
    "    field_annotation[\"uniqueItems\"] = True\n",
    "    \n",
    "    return field_annotation\n",
    "    \n",
    "    \n",
    "def add_simple_definition(row):\n",
    "    '''\n",
    "    If the feature is a simple object handled easy.\n",
    "    '''\n",
    "    field = row['field_name']\n",
    "\n",
    "    field_annotation = OrderedDict()\n",
    "    \n",
    "    # Setting type - maybe nullable:\n",
    "    field_annotation['type'] = row['type']\n",
    "    \n",
    "    # Adding description:\n",
    "    if isinstance(row['description'], str):\n",
    "        field_annotation['description'] = row['description']\n",
    "\n",
    "    # Adding minimum:\n",
    "    if not np.isnan(row['minimum']):     \n",
    "        if row['type'] == 'integer':\n",
    "            field_annotation['minimum'] = int(row['minimum'])\n",
    "        else:\n",
    "            field_annotation['minimum'] = float(row['minimum'])\n",
    "            \n",
    "                \n",
    "    # Adding maximum:\n",
    "    if not np.isnan(row['maximum']):\n",
    "        if row['type'] == 'integer':\n",
    "            field_annotation['maximum'] = int(row['maximum'])\n",
    "        else:\n",
    "            field_annotation['maximum'] = float(row['maximum'])\n",
    "            \n",
    "        \n",
    "    # Is it exclusive minimum:\n",
    "    if not np.isnan(row['exclusiveMinimum']):\n",
    "        if row['type'] == 'integer':\n",
    "            field_annotation['exclusiveMinimum'] = int(row['exclusiveMinimum'])\n",
    "        else:\n",
    "            field_annotation['exclusiveMinimum'] = float(row['exclusiveMinimum'])\n",
    "            \n",
    "        \n",
    "    # Adding pattern:\n",
    "    if isinstance(row['pattern'], str):\n",
    "        field_annotation['pattern'] = row['pattern']\n",
    "\n",
    "    # Adding examples:\n",
    "    if isinstance(row['example'], str):\n",
    "        field_annotation['examples'] = row['example'].split('|')\n",
    "        \n",
    "    # Is there a list of accepted values (might be a list of floats!!):\n",
    "    if isinstance(row['accepted_values'], str):\n",
    "        enum_values = row['accepted_values'].split('|')\n",
    "        try:\n",
    "            field_annotation['enum'] = [float(x) for x in enum_values]\n",
    "        except:\n",
    "            field_annotation['enum'] = enum_values\n",
    "        \n",
    "    return field_annotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-14T20:32:04.998605Z",
     "start_time": "2020-12-14T20:32:02.799842Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allelicRequirements\n",
      "clinicalSignificances\n",
      "clinicalUrls\n",
      "cohortPhenotypes\n",
      "diseaseCellLines\n",
      "diseaseModelAssociatedHumanPhenotypes\n",
      "diseaseModelAssociatedModelPhenotypes\n",
      "literature\n",
      "mutatedSamples\n",
      "significantDriverMethods\n",
      "textMiningSentences\n",
      "variantAminoacidDescriptions\n"
     ]
    }
   ],
   "source": [
    "# Reloading possible modifications from the xlsx file:\n",
    "fields_df = get_dataframe()\n",
    "field_mapping_df = pd.read_csv('fields_sources.tsv', sep='\\t')\n",
    "\n",
    "##\n",
    "## initialize json schema:\n",
    "##\n",
    "schema_obj = OrderedDict({\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"OpenTargets\",\n",
    "    \"description\": \"OpenTargets evidence objects\",\n",
    "    \"type\": \"object\",\n",
    "    \"oneOf\": {},\n",
    "    \"definitions\": OrderedDict()\n",
    "})\n",
    "\n",
    "##\n",
    "## Generating field definitions:\n",
    "##\n",
    "    \n",
    "# This will be updated with every row:\n",
    "root_object = OrderedDict() \n",
    "fields_df = get_dataframe()\n",
    "merged = fields_df.merge(field_mapping_df, left_on='field_name', right_on='column', how='left')\n",
    "merged.rename(columns = {'sources': 'data_source'}, inplace=True)\n",
    "\n",
    "# Adding descxription for all data sources:\n",
    "schema_obj[\"oneOf\"] = parse_data_sources(merged.loc[merged.location == 'root'])\n",
    "\n",
    "\n",
    "# Looping through all fields that are in the root of the document:\n",
    "for i, row in merged.loc[merged.location == 'root'].iterrows():\n",
    "    schema_obj['definitions'][row['field_name']] = add_definition(row, merged)\n",
    "\n",
    "# Saving object into json file:\n",
    "with open('/Users/dsuveges/repositories/json_schema/opentargets.json', 'w') as f:\n",
    "    json.dump(schema_obj, f, indent=2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T00:51:50.345309Z",
     "start_time": "2020-12-13T00:51:49.955254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've hit the request limit for your current plan. Please upgrade to continue using Scraper API, or contact support@scraperapi.com.\n"
     ]
    }
   ],
   "source": [
    "from scraper_api import ScraperAPIClient\n",
    "client = ScraperAPIClient('53403c7ec1dce6a87740bae27d78e048')\n",
    "result = client.get(url = 'http://httpbin.org/ip').text\n",
    "print(result);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T00:18:36.120734Z",
     "start_time": "2020-12-13T00:18:35.936682Z"
    }
   },
   "outputs": [],
   "source": [
    "53403c7ec1dce6a87740bae27d78e048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-13T00:18:47.370867Z",
     "start_time": "2020-12-13T00:18:47.365508Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/536.0 (KHTML, like Gecko) FxiOS/18.9t2558.0 Mobile/57Y416 Safari/536.0'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake.user_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
